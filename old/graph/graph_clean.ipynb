{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "from torch import tensor\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from utils import randexclude\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Toy dataset from Decision Transformer (Chen et. al 2021)\n",
    "class RandomWalks:\n",
    "    def __init__(self, n_nodes=20, max_length=10, n_walks=1000, p_edge=0.1, seed=1002):\n",
    "        self.n_nodes = n_nodes\n",
    "        self.n_walks = n_walks\n",
    "        self.max_length = max_length\n",
    "        self.walk_size = max_length\n",
    "        rng = np.random.RandomState(seed)\n",
    "\n",
    "        walks, rewards = [], []\n",
    "        while True:\n",
    "            self.adj = rng.rand(n_nodes, n_nodes) > (1 - p_edge)\n",
    "            np.fill_diagonal(self.adj, 0)\n",
    "            if np.all(self.adj.sum(1)): break\n",
    "\n",
    "        # terminal state\n",
    "        self.adj[0, :] = 0\n",
    "        self.adj[0, 0] = 1\n",
    "\n",
    "        self.goal = 0\n",
    "        for _ in range(n_walks):\n",
    "            node = randexclude(rng, n_nodes, self.goal)\n",
    "            walk = [node]\n",
    "\n",
    "            for istep in range(max_length-1):\n",
    "                node = rng.choice(np.nonzero(self.adj[node])[0])\n",
    "                walk.append(node)\n",
    "                if node == self.goal:\n",
    "                    break\n",
    "\n",
    "            r = th.zeros(max_length-1)\n",
    "            r[:len(walk)-1] = -1 if walk[-1] == self.goal else -100\n",
    "\n",
    "            rewards.append(r)\n",
    "            walks.append(walk)\n",
    "\n",
    "        states = []\n",
    "        attention_masks = []\n",
    "\n",
    "        for r, walk in zip(rewards, map(th.tensor, walks)):\n",
    "            attention_mask = th.zeros(max_length, dtype=int)\n",
    "            attention_mask[:len(walk)] = 1\n",
    "\n",
    "            attention_masks.append(attention_mask)\n",
    "            states.append(F.pad(walk, (0, max_length-len(walk))))\n",
    "\n",
    "        self.worstlen = self.max_length\n",
    "        self.avglen = sum(map(len, walks)) / self.n_walks\n",
    "        self.bestlen = 0\n",
    "        g = nx.from_numpy_array(self.adj, create_using=nx.DiGraph)\n",
    "        for start in set(range(self.n_nodes)) - {self.goal}:\n",
    "            try:\n",
    "                shortest_path = nx.shortest_path(g, start, self.goal)[:self.max_length]\n",
    "                self.bestlen += len(shortest_path)\n",
    "            except:\n",
    "                self.bestlen += self.max_length\n",
    "\n",
    "        self.bestlen /= self.n_nodes - 1\n",
    "\n",
    "        print(f'{self.n_walks} walks of which {(np.array([r[0] for r in rewards])==-1).mean()*100:.0f}% arrived at destination')\n",
    "\n",
    "        # disallows selecting unaccessible nodes in a graph\n",
    "        self.logit_mask = tensor(~self.adj)\n",
    "\n",
    "        self.dataset = TensorDataset(th.stack(states), th.stack(attention_masks), th.stack(rewards))\n",
    "        self.eval_dataset = TensorDataset(th.arange(1, self.n_nodes).unsqueeze(1))\n",
    "\n",
    "    def render(self):\n",
    "\n",
    "        g = nx.from_numpy_array(self.adj, create_using=nx.DiGraph)\n",
    "        pos = nx.spring_layout(g, seed=7357)\n",
    "\n",
    "        pyplot.figure(figsize=(10, 8))\n",
    "        nx.draw_networkx_edges(g, pos=pos, alpha=0.5, width=1, edge_color='#d3d3d3')\n",
    "        nx.draw_networkx_nodes(g, nodelist=set(range(len(self.adj))) - {self.goal}, pos=pos, node_size=300, node_color='orange')\n",
    "        nx.draw_networkx_nodes(g, nodelist=[self.goal], pos=pos, node_size=300, node_color='darkblue')\n",
    "        pyplot.show()\n",
    "\n",
    "    def eval(self, samples, beta):\n",
    "        narrived = 0\n",
    "        actlen = 0\n",
    "        for node in range(self.n_nodes-1):\n",
    "            for istep in range(self.max_length):\n",
    "                if samples[node, istep] == self.goal:\n",
    "                    narrived += 1\n",
    "                    break\n",
    "\n",
    "            actlen += (istep + 1) / (self.n_nodes - 1)\n",
    "\n",
    "        current = (self.worstlen - actlen)/(self.worstlen - self.bestlen)\n",
    "        average = (self.worstlen - self.avglen)/(self.worstlen - self.bestlen)\n",
    "\n",
    "        stats = { 'actlen': actlen,\n",
    "                  'avglen': self.avglen,\n",
    "                  'bestlen': self.bestlen,\n",
    "                  'worstlen': self.worstlen,\n",
    "                  'arrived': f'{narrived / (self.n_nodes-1) * 100:.0f}%',\n",
    "                  'optimal': f'{current*100:.0f}% > {average*100:.0f}%' }\n",
    "\n",
    "        return -actlen, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 walks of which 80% arrived at destination\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x560 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Walk: tensor([11,  2,  4, 15, 14,  2,  4, 16,  0,  0])\n",
      "First Walk Reward: tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  0.])\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of RandomWalks\n",
    "n_nodes = 20\n",
    "max_length = 10\n",
    "n_walks = 1000\n",
    "p_edge = 0.1\n",
    "seed = 1002\n",
    "\n",
    "random_walks = RandomWalks(n_nodes=n_nodes, max_length=max_length, n_walks=n_walks, p_edge=p_edge, seed=seed)\n",
    "\n",
    "# Access the dataset containing the walks and rewards\n",
    "dataset = random_walks.dataset\n",
    "\n",
    "# Visualize the graph\n",
    "random_walks.render()\n",
    "\n",
    "# Example: Access the first walk and its reward\n",
    "first_walk, _, first_reward = dataset[0]\n",
    "print(\"First Walk:\", first_walk)\n",
    "print(\"First Walk Reward:\", first_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as th\n",
    "import numpy as np\n",
    "from torch import tensor, nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, PretrainedConfig, AutoConfig\n",
    "\n",
    "from typing import NamedTuple, Tuple, Union\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "def topk_mask(xs: th.FloatTensor, k: int):\n",
    "    mintop = th.topk(xs, k)[0][:, -1].unsqueeze(-1)\n",
    "    return th.where(xs < mintop, -np.inf * th.ones_like(xs, dtype=xs.dtype), xs)\n",
    "\n",
    "class QVOutput(Tuple):\n",
    "    logits: th.FloatTensor\n",
    "    qs: th.FloatTensor\n",
    "    target_qs: th.FloatTensor\n",
    "    vs: th.FloatTensor\n",
    "    past_key_values: Tuple[th.FloatTensor]\n",
    "\n",
    "def make_head(n_embd: int, out: int):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(n_embd, n_embd * 2),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(n_embd * 2, out)\n",
    "    )\n",
    "\n",
    "class QVModel(nn.Module):\n",
    "    def __init__(self, config: Union[PretrainedConfig, str], params):\n",
    "        super().__init__()\n",
    "\n",
    "        # enable zero3 init within from_pretrained\n",
    "        if os.environ.get('DEEPSPEED_ZERO_STAGE', '0') == '3':\n",
    "            config_path = os.environ.get('DEEPSPEED_CONFIG_FILE', '')\n",
    "            if config_path:\n",
    "                _hfconfig = transformers.deepspeed.HfDeepSpeedConfig(config_path)\n",
    "\n",
    "        if isinstance(config, PretrainedConfig):\n",
    "            self.gpt = AutoModelForCausalLM.from_config(config)\n",
    "        else:\n",
    "            self.gpt = AutoModelForCausalLM.from_pretrained(config)\n",
    "\n",
    "        if hasattr(self.gpt.config, 'hidden_size'):\n",
    "            self.n_embd = self.gpt.config.hidden_size\n",
    "        else:\n",
    "            self.n_embd = self.gpt.config.n_embd\n",
    "        self.vocab_size = self.gpt.config.vocab_size\n",
    "\n",
    "        self.v_head = make_head(self.n_embd, 1)\n",
    "        self.q1_head = make_head(self.n_embd, self.vocab_size)\n",
    "        self.target_q1_head = deepcopy(self.q1_head)\n",
    "        self.target_q1_head.requires_grad_(False)\n",
    "\n",
    "        self.tau = params['tau']\n",
    "        self.alpha = params['alpha']\n",
    "        self.gamma = params['gamma']\n",
    "        self.awac_scale = params['awac_scale']\n",
    "        self.cql_scale = params['cql_scale']\n",
    "        self.two_qs = params['two_qs']\n",
    "\n",
    "        if self.two_qs:\n",
    "            self.q2_head = make_head(self.n_embd, self.vocab_size)\n",
    "            self.target_q2_head = deepcopy(self.q2_head)\n",
    "            self.target_q2_head.requires_grad_(False)\n",
    "\n",
    "    def forward(self, **x):\n",
    "        if hasattr(self.gpt, 'gpt_neox'):\n",
    "            out = self.gpt.gpt_neox(**x)\n",
    "        else:\n",
    "            out = self.gpt.transformer(**x)\n",
    "\n",
    "        hs = out.last_hidden_state\n",
    "\n",
    "        if self.two_qs:\n",
    "            qs = (self.q1_head(hs), self.q2_head(hs))\n",
    "            target_qs = (self.target_q1_head(hs), self.target_q2_head(hs))\n",
    "        else:\n",
    "            qs = self.q1_head(hs)\n",
    "            target_qs = self.target_q1_head(hs)\n",
    "\n",
    "        if hasattr(self.gpt, 'gpt_neox'):\n",
    "            logits = self.gpt.embed_out(hs)\n",
    "        else:\n",
    "            logits = self.gpt.lm_head(hs)\n",
    "\n",
    "        return QVOutput((logits, qs, target_qs, self.v_head(hs), out.past_key_values))\n",
    "\n",
    "    def loss(self, batch):\n",
    "        tokens, attn, rewards = batch\n",
    "        actions = tokens[:, 1:, None]\n",
    "        isterminal = attn[:, :-1]\n",
    "\n",
    "        logits, qs, target_qs, vs, _ = self(input_ids=tokens, attention_mask=attn)\n",
    "        bsize, ntokens, dsize = logits.shape\n",
    "\n",
    "        if self.two_qs:\n",
    "            Q1 = qs[0][:, :-1].gather(-1, actions).squeeze(-1)\n",
    "            Q2 = qs[1][:, :-1].gather(-1, actions).squeeze(-1)\n",
    "\n",
    "            targetQ1 = target_qs[0][:, :-1].gather(-1, actions).squeeze(-1).detach()\n",
    "            targetQ2 = target_qs[1][:, :-1].gather(-1, actions).squeeze(-1).detach()\n",
    "            targetQ = th.minimum(targetQ1, targetQ2)\n",
    "        else:\n",
    "            Q = qs[:, :-1].gather(-1, actions).squeeze(-1)\n",
    "            targetQ = target_qs[:, :-1].gather(-1, actions).squeeze(-1).detach()\n",
    "\n",
    "        n_nonterminal = max(1, isterminal.sum())\n",
    "        V = vs[:, 1:].squeeze() * isterminal\n",
    "        Q_ = rewards + self.gamma * V\n",
    "\n",
    "        if self.two_qs:\n",
    "            loss_q1 = ((Q1 - Q_.detach()) * isterminal).pow(2).sum() / n_nonterminal\n",
    "            loss_q2 = ((Q2 - Q_.detach()) * isterminal).pow(2).sum() / n_nonterminal\n",
    "            loss_q = loss_q1 + loss_q2\n",
    "        else:\n",
    "            loss_q = ((Q - Q_.detach()) * isterminal).pow(2).sum() / n_nonterminal\n",
    "\n",
    "        loss_v = (((targetQ >= V).int() * self.tau * (targetQ - V).pow(2) + (targetQ < V).int() * (1 - self.tau) * (targetQ - V).pow(2)) * isterminal).sum() / n_nonterminal\n",
    "\n",
    "        if self.two_qs:\n",
    "            loss_cql_q1 = (F.cross_entropy(qs[0][:, :-1].reshape(-1, dsize), actions.reshape(-1), reduction='none').reshape(bsize, ntokens-1) * isterminal).sum() / n_nonterminal\n",
    "            loss_cql_q2 = (F.cross_entropy(qs[1][:, :-1].reshape(-1, dsize), actions.reshape(-1), reduction='none').reshape(bsize, ntokens-1) * isterminal).sum() / n_nonterminal\n",
    "            loss_cql = loss_cql_q1 + loss_cql_q2\n",
    "        else:\n",
    "            loss_cql = (F.cross_entropy(qs[:, :-1].reshape(-1, dsize), actions.reshape(-1), reduction='none').reshape(bsize, ntokens-1) * isterminal).sum() / n_nonterminal\n",
    "\n",
    "        loss_awac = (F.cross_entropy(logits[:, :-1].reshape(-1, dsize), actions.reshape(-1), reduction='none').reshape(bsize, ntokens-1) * isterminal).sum() / n_nonterminal\n",
    "\n",
    "        loss = loss_q + loss_v + self.cql_scale * loss_cql + self.awac_scale * loss_awac\n",
    "        stats = {\n",
    "            k: v for k, v in locals().items() if k in\n",
    "            ['loss', 'loss_v', 'loss_q', 'loss_cql', 'loss_awac']\n",
    "        }\n",
    "\n",
    "        return loss, stats\n",
    "\n",
    "    def _sync_target_q_heads(self, alpha):\n",
    "        for target_param, copy_param in zip(self.target_q1_head.parameters(), self.q1_head.parameters()):\n",
    "            target_param.data.copy_((alpha * copy_param.data) + (1.0 - alpha) * target_param.data)\n",
    "\n",
    "        if self.two_qs:\n",
    "            for target_param, copy_param in zip(self.target_q2_head.parameters(), self.q2_head.parameters()):\n",
    "                target_param.data.copy_((alpha * copy_param.data) + (1.0 - alpha) * target_param.data)\n",
    "\n",
    "    @th.inference_mode()\n",
    "    def sample(self, query, beta=1, max_length=32, temperature=1, top_k=20, logit_mask=None, logs=True, eos_token_id=50256):\n",
    "        input = query.clone()\n",
    "        past_key_values = None\n",
    "        tensors = defaultdict(list)\n",
    "\n",
    "        finished = th.zeros(input.shape[0], 1, dtype=th.long, device=query.device)\n",
    "\n",
    "        for _ in range(max_length-1):\n",
    "            logits, _, target_qs, vs, past_key_values = self.forward(input_ids=input, past_key_values=past_key_values)\n",
    "\n",
    "            if self.two_qs:\n",
    "                qs = th.minimum(target_qs[0][:, -1], target_qs[1][:, -1])\n",
    "            else:\n",
    "                qs = target_qs[:, -1]\n",
    "\n",
    "            logits = logits[:, -1]\n",
    "\n",
    "            if logit_mask is not None:\n",
    "                logits[th.where(logit_mask[input[:, -1]])] = -np.inf\n",
    "\n",
    "            adv = qs - vs[:, -1, :]\n",
    "            pi = F.log_softmax(logits, -1)\n",
    "            modpi = topk_mask(pi + beta * adv, top_k)\n",
    "            ps = F.softmax(modpi / temperature, -1)\n",
    "\n",
    "            tokens = th.multinomial(ps, 1)\n",
    "            tokens = (1 - finished) * tokens + finished * eos_token_id\n",
    "\n",
    "            query = th.hstack((query, tokens))\n",
    "\n",
    "            input = tokens\n",
    "            finished = (tokens == eos_token_id).long()\n",
    "\n",
    "            if logs:\n",
    "                tensors['qs'].append(qs)\n",
    "                tensors['vs'].append(vs)\n",
    "                tensors['adv'].append(adv)\n",
    "\n",
    "        stats = {}\n",
    "        for name, xs in tensors.items():\n",
    "            xs = th.vstack(xs)\n",
    "            stats.update({\n",
    "                f'{name}-min': xs.min(),\n",
    "                f'{name}-max': xs.max(),\n",
    "                f'{name}-std': xs.std(),\n",
    "                f'{name}-avg': xs.mean(),\n",
    "            })\n",
    "\n",
    "        return query, stats\n",
    "\n",
    "    @property\n",
    "    def dummy_inputs(self):\n",
    "        return {'input_ids': th.ones(1, 1, device=self.gpt.device, dtype=th.long)}\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.gpt.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import torch as th\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Config\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np\n",
    "\n",
    "th.set_printoptions(sci_mode=False)\n",
    "\n",
    "def main(**args):\n",
    "    task = 'RandomWalks'\n",
    "    config = yaml.safe_load(open('config.yaml'))[task]\n",
    "    config.update(args)\n",
    "\n",
    "    device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "    th.manual_seed(config['seed'])\n",
    "\n",
    "    data = RandomWalks(seed=config['seed'])\n",
    "    gptconfig = GPT2Config(**config['gptconfig'], vocab_size=data.n_nodes)\n",
    "    model = QVModel(gptconfig, config).to(device)\n",
    "\n",
    "    for m in model.gpt.transformer.h[:-config['n_layers_unfrozen']]:\n",
    "        m.requires_grad_(False)\n",
    "\n",
    "    train_dataloader = DataLoader(data.dataset, batch_size=config['batch_size'])\n",
    "    opt = th.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=config['lr'], betas=config['opt_betas'])\n",
    "\n",
    "    total_steps = int(config['n_epochs'] * (len(data.dataset) // config['batch_size']))\n",
    "    model.train()\n",
    "\n",
    "    for _ in trange(total_steps):\n",
    "        for batch in train_dataloader:\n",
    "            batch = [item.to(device) for item in batch]\n",
    "            loss, stats = model.loss(batch)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            # if stats:\n",
    "            #     print(stats)  # Optionally, print stats for each batch\n",
    "\n",
    "    return model, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 walks of which 18% arrived at destination\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:53<00:00,  1.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(QVModel(\n",
       "   (gpt): GPT2LMHeadModel(\n",
       "     (transformer): GPT2Model(\n",
       "       (wte): Embedding(20, 144)\n",
       "       (wpe): Embedding(1024, 144)\n",
       "       (drop): Dropout(p=0.1, inplace=False)\n",
       "       (h): ModuleList(\n",
       "         (0-5): 6 x GPT2Block(\n",
       "           (ln_1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "           (attn): GPT2Attention(\n",
       "             (c_attn): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "             (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (ln_2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): GPT2MLP(\n",
       "             (c_fc): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (act): NewGELUActivation()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (ln_f): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "     )\n",
       "     (lm_head): Linear(in_features=144, out_features=20, bias=False)\n",
       "   )\n",
       "   (v_head): Sequential(\n",
       "     (0): Linear(in_features=144, out_features=288, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=288, out_features=1, bias=True)\n",
       "   )\n",
       "   (q1_head): Sequential(\n",
       "     (0): Linear(in_features=144, out_features=288, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=288, out_features=20, bias=True)\n",
       "   )\n",
       "   (target_q1_head): Sequential(\n",
       "     (0): Linear(in_features=144, out_features=288, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=288, out_features=20, bias=True)\n",
       "   )\n",
       "   (q2_head): Sequential(\n",
       "     (0): Linear(in_features=144, out_features=288, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=288, out_features=20, bias=True)\n",
       "   )\n",
       "   (target_q2_head): Sequential(\n",
       "     (0): Linear(in_features=144, out_features=288, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=288, out_features=20, bias=True)\n",
       "   )\n",
       " ),\n",
       " <__main__.RandomWalks at 0x1756d4a90>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = {'debug': True}\n",
    "main(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 walks of which 37% arrived at destination\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:49<00:00,  1.82it/s]\n",
      "/var/folders/wg/cdt_cw_5265_z_gwnxlf0tv80000gn/T/ipykernel_17189/4108513398.py:36: DeprecationWarning: Calling nonzero on 0d arrays is deprecated, as it behaves surprisingly. Use `atleast_1d(cond).nonzero()` if the old behavior was intended. If the context of this warning is of the form `arr[nonzero(cond)]`, just use `arr[cond]`.\n",
      "  length = np.where(path.numpy() == data.goal)[0][0] if data.goal in path.numpy() else data.max_length\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 walks of which 74% arrived at destination\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 walks of which 29% arrived at destination\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:56<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 walks of which 51% arrived at destination\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:51<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 walks of which 7% arrived at destination\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:48<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 walks of which 69% arrived at destination\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:48<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 walks of which 92% arrived at destination\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:49<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 walks of which 50% arrived at destination\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:49<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 walks of which 15% arrived at destination\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:48<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 walks of which 32% arrived at destination\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:48<00:00,  1.84it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1050x560 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch as th\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# Assuming model, data, and main function are defined as before\n",
    "optimal_lengths = []\n",
    "sampled_lengths = []\n",
    "iql_lengths = []\n",
    "\n",
    "for seed in range(10):\n",
    "    model, data = main(seed=seed, debug=True)\n",
    "    model.eval()\n",
    "\n",
    "    g = nx.from_numpy_array(data.adj, create_using=nx.DiGraph)\n",
    "\n",
    "    # Optimal paths calculation\n",
    "    for start in range(data.n_nodes):\n",
    "        if start != data.goal:\n",
    "            try:\n",
    "                shortest_path = nx.shortest_path(g, start, data.goal)\n",
    "                optimal_lengths.append(len(shortest_path) - 1)\n",
    "            except nx.NetworkXNoPath:\n",
    "                optimal_lengths.append(data.max_length)\n",
    "\n",
    "    # ILQL (model-generated) paths calculation\n",
    "    starts = th.arange(data.n_nodes).unsqueeze(1).to(model.device)\n",
    "    paths, _ = model.sample(starts, max_length=data.max_length, logit_mask=tensor(~data.adj), beta=10)\n",
    "    for path in paths.cpu().numpy():\n",
    "        iql_lengths.append(np.where(path == data.goal)[0][0] if data.goal in path else data.max_length)\n",
    "\n",
    "    # Random walk paths calculation\n",
    "    for batch in data.dataset:\n",
    "        for path in batch[0]:\n",
    "            length = np.where(path.numpy() == data.goal)[0][0] if data.goal in path.numpy() else data.max_length\n",
    "            sampled_lengths.append(length)\n",
    "\n",
    "fontcolor = '#444'\n",
    "# Bar sizes and colors for the plot\n",
    "barsize = 0.36\n",
    "iql_color = '#99a3fd'\n",
    "opt_color = '#f2ad48'\n",
    "random_color = 'lightgray'\n",
    "\n",
    "matplotlib.rcParams['text.color'] = fontcolor\n",
    "matplotlib.rcParams['axes.labelcolor'] = fontcolor\n",
    "matplotlib.rcParams['xtick.color'] = fontcolor\n",
    "matplotlib.rcParams['ytick.color'] = fontcolor\n",
    "matplotlib.rcParams['xtick.labelcolor'] = fontcolor\n",
    "matplotlib.rcParams['ytick.labelcolor'] = fontcolor\n",
    "matplotlib.rcParams['xtick.labelcolor'] = fontcolor\n",
    "\n",
    "matplotlib.rcParams[\"font.family\"] = \"Futura\"\n",
    "matplotlib.rcParams[\"font.size\"] = 15\n",
    "matplotlib.rcParams[\"xtick.labelsize\"] = 20\n",
    "matplotlib.rcParams[\"ytick.labelsize\"] = 20\n",
    "matplotlib.rcParams[\"figure.titlesize\"] = 12\n",
    "matplotlib.rcParams[\"figure.figsize\"] = 15, 8\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "matplotlib.rcParams['figure.dpi'] = 70\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Adjusted bin range to include all lengths from 1 to max_length, plus one additional bin\n",
    "bins = np.arange(1, data.max_length + 2)\n",
    "\n",
    "# Histogram calculation\n",
    "optimal_hist = np.histogram(optimal_lengths, bins=bins, density=True)[0]\n",
    "sampled_hist = np.histogram(sampled_lengths, bins=bins, density=True)[0]\n",
    "iql_hist = np.histogram(iql_lengths, bins=bins, density=True)[0]\n",
    "\n",
    "# Adjust x-ticks to properly align with the bars\n",
    "x_ticks = np.arange(1, data.max_length + 1)\n",
    "\n",
    "# Plotting histograms\n",
    "plt.bar(x_ticks - barsize/1.5, optimal_hist, width=barsize, label='Shortest Path', color=opt_color, zorder=2)\n",
    "plt.bar(x_ticks, iql_hist, width=barsize, label='ILQL', color=iql_color, zorder=3)\n",
    "plt.bar(x_ticks + barsize/1.5, sampled_hist, width=barsize, label='Random Walk', color=random_color, zorder=1)\n",
    "\n",
    "# Adjusting x-axis labels to properly reflect the bins\n",
    "plt.xticks(np.arange(1, data.max_length + 2), list(np.arange(1, data.max_length + 1)) + ['∞'])\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "plt.xticks(np.arange(1, data.max_length+2), list(np.arange(1, data.max_length+1)) + ['∞'])\n",
    "plt.xlabel('# of Steps to Goal', fontsize=22, color=fontcolor, labelpad=20)\n",
    "plt.ylabel('Proportion of Paths', fontsize=22, color=fontcolor, labelpad=20)\n",
    "\n",
    "plt.savefig('graph_plot.svg')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please let this work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Invalid normalization_type passed in LayerNorm\n",
      "WARNING:root:Invalid normalization_type passed in LayerNorm\n",
      "WARNING:root:Invalid normalization_type passed in LayerNorm\n",
      "WARNING:root:Invalid normalization_type passed in LayerNorm\n",
      "WARNING:root:Invalid normalization_type passed in LayerNorm\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tensors used as indices must be long, int, byte or bool tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 91\u001b[0m\n\u001b[1;32m     88\u001b[0m timesteps \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(config\u001b[39m.\u001b[39mmax_length)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     90\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m logits \u001b[39m=\u001b[39m model(states, actions, rewards, timesteps)\n\u001b[1;32m     92\u001b[0m next_action \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict_next_act\n",
      "File \u001b[0;32m~/miniconda3/envs/honours/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/honours/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[100], line 70\u001b[0m, in \u001b[0;36mDecisionTransformer.forward\u001b[0;34m(self, states, actions, rewards, timesteps)\u001b[0m\n\u001b[1;32m     67\u001b[0m embeddings \u001b[39m=\u001b[39m rearrange(embeddings, \u001b[39m\"\u001b[39m\u001b[39mb s d -> s b d\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[39m# Transformer\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m transformer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(embeddings)\n\u001b[1;32m     72\u001b[0m \u001b[39m# Action prediction\u001b[39;00m\n\u001b[1;32m     73\u001b[0m action_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_head(transformer_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/honours/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/honours/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/honours/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:527\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[39mwith\u001b[39;00m utils\u001b[39m.\u001b[39mLocallyOverridenDefaults(\n\u001b[1;32m    519\u001b[0m     \u001b[39mself\u001b[39m, prepend_bos\u001b[39m=\u001b[39mprepend_bos, padding_side\u001b[39m=\u001b[39mpadding_side\n\u001b[1;32m    520\u001b[0m ):\n\u001b[1;32m    521\u001b[0m     \u001b[39mif\u001b[39;00m start_at_layer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    522\u001b[0m         (\n\u001b[1;32m    523\u001b[0m             residual,\n\u001b[1;32m    524\u001b[0m             tokens,\n\u001b[1;32m    525\u001b[0m             shortformer_pos_embed,\n\u001b[1;32m    526\u001b[0m             attention_mask,\n\u001b[0;32m--> 527\u001b[0m         ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_to_embed(\n\u001b[1;32m    528\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    529\u001b[0m             prepend_bos\u001b[39m=\u001b[39;49mprepend_bos,\n\u001b[1;32m    530\u001b[0m             padding_side\u001b[39m=\u001b[39;49mpadding_side,\n\u001b[1;32m    531\u001b[0m             past_kv_cache\u001b[39m=\u001b[39;49mpast_kv_cache,\n\u001b[1;32m    532\u001b[0m         )\n\u001b[1;32m    533\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    534\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39minput\u001b[39m) \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mTensor\n",
      "File \u001b[0;32m~/miniconda3/envs/honours/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:317\u001b[0m, in \u001b[0;36mHookedTransformer.input_to_embed\u001b[0;34m(self, input, prepend_bos, padding_side, past_kv_cache)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39muse_hook_tokens:\n\u001b[1;32m    316\u001b[0m     tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_tokens(tokens)\n\u001b[0;32m--> 317\u001b[0m embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_embed(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed(tokens))  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mpositional_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mstandard\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    319\u001b[0m     pos_embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_pos_embed(\n\u001b[1;32m    320\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_embed(tokens, pos_offset, attention_mask)\n\u001b[1;32m    321\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/honours/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/honours/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/honours/lib/python3.10/site-packages/transformer_lens/components.py:46\u001b[0m, in \u001b[0;36mEmbed.forward\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mpost_embedding_ln:\n\u001b[1;32m     45\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_E[tokens, :])\n\u001b[0;32m---> 46\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW_E[tokens, :]\n",
      "\u001b[0;31mIndexError\u001b[0m: tensors used as indices must be long, int, byte or bool tensors"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtyping import TensorType\n",
    "from einops import rearrange\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "\n",
    "class RandomWalksTransformerConfig:\n",
    "    def __init__(self):\n",
    "        self.n_nodes = 20  # Number of nodes in the graph\n",
    "        self.d_model = 128  # Dimension of the embeddings\n",
    "        self.n_heads = 4    # Number of attention heads\n",
    "        self.n_layers = 4   # Number of transformer layers\n",
    "        self.max_length = 10  # Maximum length of a sequence\n",
    "        self.n_ctx = self.max_length * 3  # Context size\n",
    "        self.d_head = int(self.d_model / self.n_heads)  # Dimension per head\n",
    "        self.act_fn = \"relu\"  # Activation function\n",
    "\n",
    "class DecisionTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Embeddings\n",
    "        self.node_embedding = nn.Embedding(config.n_nodes, config.d_model)\n",
    "        self.position_embedding = nn.Embedding(config.max_length, config.d_model)\n",
    "        self.action_embedding = nn.Embedding(config.n_nodes + 1, config.d_model)\n",
    "        self.reward_embedding = nn.Linear(1, config.d_model)\n",
    "\n",
    "        # Transformer\n",
    "        transformer_cfg = HookedTransformerConfig(\n",
    "            n_layers=config.n_layers,\n",
    "            d_model=config.d_model,\n",
    "            d_head=config.d_head,\n",
    "            n_heads=config.n_heads,\n",
    "            d_vocab=config.n_nodes + 1,\n",
    "            d_mlp=config.d_model * 4,\n",
    "            normalization_type='LayerNorm',\n",
    "            d_vocab_out=config.d_model,\n",
    "            attention_dir='causal',\n",
    "            n_ctx=config.n_ctx,\n",
    "            act_fn=config.act_fn,\n",
    "        )\n",
    "        self.transformer = HookedTransformer(transformer_cfg)\n",
    "\n",
    "        # Output head for action prediction\n",
    "        self.action_head = nn.Linear(config.d_model, config.n_nodes)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, states, actions, rewards, timesteps):\n",
    "        batch_size, seq_length = states.shape\n",
    "\n",
    "        # Embeddings\n",
    "        state_embeddings = self.node_embedding(states)\n",
    "        position_embeddings = self.position_embedding(timesteps)\n",
    "        action_embeddings = self.action_embedding(actions)\n",
    "        reward_embeddings = self.reward_embedding(rewards.unsqueeze(-1))\n",
    "\n",
    "        # Combine embeddings\n",
    "        embeddings = state_embeddings + position_embeddings + action_embeddings + reward_embeddings\n",
    "        embeddings = rearrange(embeddings, \"b s d -> s b d\")\n",
    "\n",
    "        # Transformer\n",
    "        transformer_output = self.transformer(embeddings)\n",
    "\n",
    "        # Action prediction\n",
    "        action_logits = self.action_head(transformer_output)\n",
    "        return action_logits\n",
    "\n",
    "    def predict_next_action(self, states, actions, rewards, timesteps):\n",
    "        logits = self.forward(states, actions, rewards, timesteps)\n",
    "        return torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Example usage\n",
    "config = RandomWalksTransformerConfig()\n",
    "model = DecisionTransformer(config)\n",
    "\n",
    "# Example data (batch_size, sequence_length)\n",
    "states = torch.randint(0, config.n_nodes, (1, config.max_length))\n",
    "actions = torch.randint(0, config.n_nodes, (1, config.max_length))\n",
    "rewards = torch.randn(1, config.max_length)\n",
    "timesteps = torch.arange(config.max_length).unsqueeze(0)\n",
    "\n",
    "# Forward pass\n",
    "logits = model(states, actions, rewards, timesteps)\n",
    "next_action = model.predict_next_act"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewriting with decision transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: torch.Size([10, 1, 20])\n",
      "Values: torch.Size([10, 1])\n",
      "Predicted next node: tensor([[ 5],\n",
      "        [ 0],\n",
      "        [11],\n",
      "        [12],\n",
      "        [14],\n",
      "        [19],\n",
      "        [10],\n",
      "        [15],\n",
      "        [10],\n",
      "        [11]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesoneill/miniconda3/envs/honours/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from einops import rearrange\n",
    "\n",
    "class RandomWalksTransformerConfig:\n",
    "    \"\"\"Configuration for the RandomWalks Transformer\"\"\"\n",
    "    def __init__(self):\n",
    "        self.n_nodes = 20  # Number of nodes in the graph\n",
    "        self.d_model = 128  # Dimension of the embeddings\n",
    "        self.n_heads = 4    # Number of attention heads\n",
    "        self.n_layers = 4   # Number of transformer layers\n",
    "        self.max_length = 10  # Maximum length of a sequence\n",
    "\n",
    "class DecisionTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Embedding layers\n",
    "        self.node_embedding = nn.Embedding(config.n_nodes, config.d_model)\n",
    "        self.position_embedding = nn.Embedding(config.max_length, config.d_model)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=config.d_model, nhead=config.n_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=config.n_layers)\n",
    "\n",
    "        # Output heads\n",
    "        self.action_head = nn.Linear(config.d_model, config.n_nodes)  # Predicting the next node\n",
    "        self.value_head = nn.Linear(config.d_model, 1)  # Estimating the value\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, tokens, attn):\n",
    "        \"\"\"\n",
    "        tokens: Tensor of shape (batch_size, sequence_length)\n",
    "        attn: Tensor of shape (batch_size, sequence_length) indicating valid positions\n",
    "        \"\"\"\n",
    "        # Embeddings\n",
    "        node_embeddings = self.node_embedding(tokens)  # (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Derive positions from attention masks\n",
    "        positions = attn.cumsum(dim=1) - 1  # Calculate positions based on attn\n",
    "        position_embeddings = self.position_embedding(positions)  # (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Combine embeddings\n",
    "        x = node_embeddings + position_embeddings  # (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Transformer\n",
    "        x = rearrange(x, \"b s d -> s b d\")  # Transformer expects (sequence_length, batch_size, d_model)\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # Output heads\n",
    "        action_logits = self.action_head(x)\n",
    "        values = self.value_head(x).squeeze(-1)\n",
    "\n",
    "        return F.log_softmax(action_logits, dim=-1), values\n",
    "\n",
    "    def predict_next_node(self, tokens, attn):\n",
    "        logits, _ = self.forward(tokens, attn)\n",
    "        return torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Example usage\n",
    "config = RandomWalksTransformerConfig()\n",
    "model = DecisionTransformer(config)\n",
    "\n",
    "# Example data\n",
    "tokens = torch.randint(0, config.n_nodes, (1, config.max_length))  # Random sequence of nodes\n",
    "\n",
    "# Simulating attention masks (1s for valid positions, 0s for padding)\n",
    "attn = torch.ones_like(tokens)  # Assuming all positions are valid for this example\n",
    "\n",
    "# Forward pass\n",
    "logits, values = model(tokens, attn)\n",
    "next_node = model.predict_next_node(tokens, attn)\n",
    "\n",
    "# Print results\n",
    "print(\"Logits:\", logits.shape)\n",
    "print(\"Values:\", values.shape)\n",
    "print(\"Predicted next node:\", next_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import cross_entropy\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "def main(**args):\n",
    "    # Load and update configuration\n",
    "    config = yaml.safe_load(open('config.yaml'))['RandomWalks']\n",
    "    config.update(args)\n",
    "\n",
    "    # Initialize device, data, and model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data = RandomWalks(seed=config['seed'])  # Assuming RandomWalks dataset is defined\n",
    "    model_config = RandomWalksTransformerConfig()\n",
    "    model = DecisionTransformer(model_config).to(device)\n",
    "\n",
    "    # DataLoader and optimizer\n",
    "    train_dataloader = DataLoader(data.dataset, batch_size=config['batch_size'])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "\n",
    "    # Training loop\n",
    "    for i in tqdm(range(config['n_epochs']), desc=\"Training\"):\n",
    "        for batch in train_dataloader:\n",
    "            tokens, attn, rewards = batch\n",
    "            tokens = tokens.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits, _ = model(tokens[:, :-1], attn[:, :-1])  # Use tokens and attn excluding the last element\n",
    "\n",
    "            # Prepare actions\n",
    "            actions = tokens[:, 1:].contiguous().to(device)  # Shift tokens by one for next node prediction\n",
    "            loss = compute_loss(logits, actions)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print every 10 epochs\n",
    "        # if i % 10 == 0:\n",
    "        #     print(f\"Epoch {i}, Loss: {loss.item()}\")\n",
    "\n",
    "    return model, data\n",
    "\n",
    "def compute_loss(logits, actions):\n",
    "    # Print shapes\n",
    "    # print(\"Logits:\", logits.shape)\n",
    "    # print(\"Actions:\", actions.shape)\n",
    "    \n",
    "    logits = logits.reshape(-1, logits.size(-1))  # Reshape to [batch_size * sequence_length, n_nodes]\n",
    "    actions = actions.view(-1)  # Flatten actions\n",
    "    return cross_entropy(logits, actions)\n",
    "\n",
    "# Assuming RandomWalks, RandomWalksTransformerConfig, and DecisionTransformer are defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesoneill/miniconda3/envs/honours/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 walks of which 18% arrived at destination\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 1/100 [00:00<01:11,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3.413510799407959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 11/100 [00:07<01:03,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.67281174659729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 21/100 [00:15<00:57,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 2.6557464599609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 31/100 [00:22<00:50,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Loss: 2.6622045040130615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████      | 41/100 [00:29<00:43,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, Loss: 2.6485483646392822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████     | 51/100 [00:37<00:35,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Loss: 2.63748836517334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████    | 61/100 [00:44<00:28,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60, Loss: 2.6365609169006348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|███████   | 71/100 [00:51<00:21,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70, Loss: 2.6291348934173584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|████████  | 81/100 [00:58<00:13,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80, Loss: 2.631920337677002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████ | 91/100 [01:06<00:06,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90, Loss: 2.6239607334136963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:12<00:00,  1.37it/s]\n"
     ]
    }
   ],
   "source": [
    "model, data = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 walks of which 37% arrived at destination\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2/2 [00:01<00:00,  1.36it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m             optimal_lengths\u001b[39m.\u001b[39mappend(data\u001b[39m.\u001b[39mmax_length)\n\u001b[1;32m     50\u001b[0m \u001b[39m# ILQL (model-generated) paths calculation\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m model_generated_paths \u001b[39m=\u001b[39m sample_paths(model, data)\n\u001b[1;32m     52\u001b[0m \u001b[39mfor\u001b[39;00m path \u001b[39min\u001b[39;00m model_generated_paths:\n\u001b[1;32m     53\u001b[0m     iql_lengths\u001b[39m.\u001b[39mappend(\u001b[39mlen\u001b[39m(path) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m path[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m data\u001b[39m.\u001b[39mgoal \u001b[39melse\u001b[39;00m data\u001b[39m.\u001b[39mmax_length)\n",
      "Cell \u001b[0;32mIn[77], line 20\u001b[0m, in \u001b[0;36msample_paths\u001b[0;34m(model, data)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m# Ensure positions are within the valid range\u001b[39;00m\n\u001b[1;32m     18\u001b[0m current_positions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclamp(current_positions, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39mmax_length \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m next_node \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict_next_node(current_nodes, current_positions)[\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     21\u001b[0m \u001b[39mif\u001b[39;00m next_node \u001b[39m==\u001b[39m data\u001b[39m.\u001b[39mgoal:\n\u001b[1;32m     22\u001b[0m     path\u001b[39m.\u001b[39mappend(next_node)\n",
      "Cell \u001b[0;32mIn[60], line 66\u001b[0m, in \u001b[0;36mDecisionTransformer.predict_next_node\u001b[0;34m(self, tokens, attn)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_next_node\u001b[39m(\u001b[39mself\u001b[39m, tokens, attn):\n\u001b[0;32m---> 66\u001b[0m     logits, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(tokens, attn)\n\u001b[1;32m     67\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39margmax(logits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[60], line 50\u001b[0m, in \u001b[0;36mDecisionTransformer.forward\u001b[0;34m(self, tokens, attn)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39m# Derive positions from attention masks\u001b[39;00m\n\u001b[1;32m     49\u001b[0m positions \u001b[39m=\u001b[39m attn\u001b[39m.\u001b[39mcumsum(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m  \u001b[39m# Calculate positions based on attn\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m position_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mposition_embedding(positions)  \u001b[39m# (batch_size, sequence_length, d_model)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39m# Combine embeddings\u001b[39;00m\n\u001b[1;32m     53\u001b[0m x \u001b[39m=\u001b[39m node_embeddings \u001b[39m+\u001b[39m position_embeddings  \u001b[39m# (batch_size, sequence_length, d_model)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/honours/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/honours/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/honours/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/miniconda3/envs/honours/lib/python3.10/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "def sample_paths(model, data):\n",
    "    device = next(model.parameters()).device\n",
    "    sampled_paths = []\n",
    "\n",
    "    for start_node in range(data.n_nodes):\n",
    "        path = [start_node]\n",
    "        for step in range(1, data.max_length):\n",
    "            current_nodes = torch.tensor([path], dtype=torch.long, device=device)\n",
    "            current_positions = torch.arange(start=step, end=step + len(path), dtype=torch.long, device=device).unsqueeze(0)\n",
    "            \n",
    "            # Ensure positions are within the valid range\n",
    "            current_positions = torch.clamp(current_positions, max=data.max_length - 1)\n",
    "\n",
    "            next_node = model.predict_next_node(current_nodes, current_positions)[0, -1].item()\n",
    "            if next_node == data.goal:\n",
    "                path.append(next_node)\n",
    "                break\n",
    "            elif len(path) < data.max_length:\n",
    "                path.append(next_node)\n",
    "\n",
    "        sampled_paths.append(path)\n",
    "    return sampled_paths\n",
    "\n",
    "# Assuming model, data, and main function are defined as before\n",
    "optimal_lengths = []\n",
    "sampled_lengths = []\n",
    "iql_lengths = []\n",
    "\n",
    "for seed in range(2):\n",
    "    model, data = main(seed=seed, debug=True)\n",
    "    model.eval()\n",
    "\n",
    "    g = nx.from_numpy_array(data.adj, create_using=nx.DiGraph)\n",
    "\n",
    "    # Optimal paths calculation\n",
    "    for start in range(data.n_nodes):\n",
    "        if start != data.goal:\n",
    "            try:\n",
    "                shortest_path = nx.shortest_path(g, start, data.goal)\n",
    "                optimal_lengths.append(len(shortest_path) - 1)\n",
    "            except nx.NetworkXNoPath:\n",
    "                optimal_lengths.append(data.max_length)\n",
    "\n",
    "    # ILQL (model-generated) paths calculation\n",
    "    model_generated_paths = sample_paths(model, data)\n",
    "    for path in model_generated_paths:\n",
    "        iql_lengths.append(len(path) - 1 if path[-1] == data.goal else data.max_length)\n",
    "\n",
    "    # Random walk paths calculation\n",
    "    for batch in data.dataset:\n",
    "        for path in batch[0]:\n",
    "            length = np.where(path.numpy() == data.goal)[0][0] if data.goal in path.numpy() else data.max_length\n",
    "            sampled_lengths.append(length)\n",
    "\n",
    "fontcolor = '#444'\n",
    "# Bar sizes and colors for the plot\n",
    "barsize = 0.36\n",
    "iql_color = '#99a3fd'\n",
    "opt_color = '#f2ad48'\n",
    "random_color = 'lightgray'\n",
    "\n",
    "matplotlib.rcParams['text.color'] = fontcolor\n",
    "matplotlib.rcParams['axes.labelcolor'] = fontcolor\n",
    "matplotlib.rcParams['xtick.color'] = fontcolor\n",
    "matplotlib.rcParams['ytick.color'] = fontcolor\n",
    "matplotlib.rcParams['xtick.labelcolor'] = fontcolor\n",
    "matplotlib.rcParams['ytick.labelcolor'] = fontcolor\n",
    "matplotlib.rcParams['xtick.labelcolor'] = fontcolor\n",
    "\n",
    "matplotlib.rcParams[\"font.family\"] = \"Futura\"\n",
    "matplotlib.rcParams[\"font.size\"] = 15\n",
    "matplotlib.rcParams[\"xtick.labelsize\"] = 20\n",
    "matplotlib.rcParams[\"ytick.labelsize\"] = 20\n",
    "matplotlib.rcParams[\"figure.titlesize\"] = 12\n",
    "matplotlib.rcParams[\"figure.figsize\"] = 15, 8\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "matplotlib.rcParams['figure.dpi'] = 70\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Adjusted bin range to include all lengths from 1 to max_length, plus one additional bin\n",
    "bins = np.arange(1, data.max_length + 2)\n",
    "\n",
    "# Histogram calculation\n",
    "optimal_hist = np.histogram(optimal_lengths, bins=bins, density=True)[0]\n",
    "sampled_hist = np.histogram(sampled_lengths, bins=bins, density=True)[0]\n",
    "iql_hist = np.histogram(iql_lengths, bins=bins, density=True)[0]\n",
    "\n",
    "# Adjust x-ticks to properly align with the bars\n",
    "x_ticks = np.arange(1, data.max_length + 1)\n",
    "\n",
    "# Plotting histograms\n",
    "plt.bar(x_ticks - barsize/1.5, optimal_hist, width=barsize, label='Shortest Path', color=opt_color, zorder=2)\n",
    "plt.bar(x_ticks, iql_hist, width=barsize, label='ILQL', color=iql_color, zorder=3)\n",
    "plt.bar(x_ticks + barsize/1.5, sampled_hist, width=barsize, label='Random Walk', color=random_color, zorder=1)\n",
    "\n",
    "# Adjusting x-axis labels to properly reflect the bins\n",
    "plt.xticks(np.arange(1, data.max_length + 2), list(np.arange(1, data.max_length + 1)) + ['∞'])\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "plt.xticks(np.arange(1, data.max_length+2), list(np.arange(1, data.max_length+1)) + ['∞'])\n",
    "plt.xlabel('# of Steps to Goal', fontsize=22, color=fontcolor, labelpad=20)\n",
    "plt.ylabel('Proportion of Paths', fontsize=22, color=fontcolor, labelpad=20)\n",
    "\n",
    "plt.savefig('graph_plot.svg')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "act_fn must be specified for non-attn-only models",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 72\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[39m# Additional methods for sampling, reward prediction, etc.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m \u001b[39m# Configuration and model initialization\u001b[39;00m\n\u001b[1;32m     71\u001b[0m config \u001b[39m=\u001b[39m RandomWalksTransformerConfig()\n\u001b[0;32m---> 72\u001b[0m model \u001b[39m=\u001b[39m DecisionTransformer(config)\n\u001b[1;32m     74\u001b[0m \u001b[39m# Example data\u001b[39;00m\n\u001b[1;32m     75\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n",
      "Cell \u001b[0;32mIn[81], line 26\u001b[0m, in \u001b[0;36mDecisionTransformer.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_embedding \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39m1\u001b[39m, config\u001b[39m.\u001b[39md_model)\n\u001b[1;32m     25\u001b[0m \u001b[39m# Transformer\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m transformer_cfg \u001b[39m=\u001b[39m HookedTransformerConfig(\n\u001b[1;32m     27\u001b[0m     n_layers\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mn_layers,\n\u001b[1;32m     28\u001b[0m     d_model\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49md_model,\n\u001b[1;32m     29\u001b[0m     d_head\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49md_model \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m config\u001b[39m.\u001b[39;49mn_heads,  \u001b[39m# Adjust d_head accordingly\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m     n_heads\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mn_heads,\n\u001b[1;32m     31\u001b[0m     d_mlp\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49md_model \u001b[39m*\u001b[39;49m \u001b[39m4\u001b[39;49m,\n\u001b[1;32m     32\u001b[0m     normalization_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mLayerNorm\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     33\u001b[0m     d_vocab_out\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49md_model,\n\u001b[1;32m     34\u001b[0m     attention_dir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcausal\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     35\u001b[0m     n_ctx\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mmax_length\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer \u001b[39m=\u001b[39m HookedTransformer(transformer_cfg)\n\u001b[1;32m     39\u001b[0m \u001b[39m# Output head for action prediction\u001b[39;00m\n",
      "File \u001b[0;32m<string>:49\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, n_layers, d_model, n_ctx, d_head, model_name, n_heads, d_mlp, act_fn, d_vocab, eps, use_attn_result, use_attn_scale, use_split_qkv_input, use_hook_mlp_in, use_attn_in, use_local_attn, original_architecture, from_checkpoint, checkpoint_index, checkpoint_label_type, checkpoint_value, tokenizer_name, window_size, attn_types, init_mode, normalization_type, device, n_devices, attention_dir, attn_only, seed, initializer_range, init_weights, scale_attn_by_inverse_layer_idx, positional_embedding_type, final_rms, d_vocab_out, parallel_attn_mlp, rotary_dim, n_params, use_hook_tokens, gated_mlp, default_prepend_bos, dtype, tokenizer_prepends_bos, post_embedding_ln)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/honours/lib/python3.10/site-packages/transformer_lens/HookedTransformerConfig.py:224\u001b[0m, in \u001b[0;36mHookedTransformerConfig.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_mlp \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m         \u001b[39m# For some reason everyone hard codes in this hyper-parameter!\u001b[39;00m\n\u001b[1;32m    222\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_mlp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model \u001b[39m*\u001b[39m \u001b[39m4\u001b[39m\n\u001b[1;32m    223\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[0;32m--> 224\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    225\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mact_fn must be specified for non-attn-only models\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    226\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    227\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact_fn \u001b[39min\u001b[39;00m SUPPORTED_ACTIVATIONS\n\u001b[1;32m    228\u001b[0m     ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mact_fn=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact_fn\u001b[39m}\u001b[39;00m\u001b[39m must be one of \u001b[39m\u001b[39m{\u001b[39;00mSUPPORTED_ACTIVATIONS\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitializer_range \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    230\u001b[0m     \u001b[39m# Roughly copy the GPT-2 value, but proportional to sqrt(1/d_model)\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: act_fn must be specified for non-attn-only models"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "from einops import rearrange\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    d_model: int = 128\n",
    "    n_heads: int = 4\n",
    "    d_mlp: int = 256\n",
    "    n_layers: int = 2\n",
    "    n_ctx: int = 2\n",
    "    layer_norm: Optional[str] = None\n",
    "    gated_mlp: bool = False\n",
    "    activation_fn: str = \"relu\"\n",
    "    state_embedding_type: str = \"grid\"\n",
    "    time_embedding_type: str = \"embedding\"\n",
    "    seed: int = 1\n",
    "    device: str = \"cpu\"\n",
    "\n",
    "class DecisionTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.transformer_config = config\n",
    "\n",
    "        # Embeddings\n",
    "        self.node_embedding = nn.Embedding(config.n_nodes, config.d_model)\n",
    "        self.time_embedding = nn.Embedding(config.max_length, config.d_model)\n",
    "        self.reward_embedding = nn.Linear(1, config.d_model)\n",
    "\n",
    "        # Transformer\n",
    "        transformer_cfg = HookedTransformerConfig(\n",
    "            n_layers=config.n_layers,\n",
    "            d_model=config.d_model,\n",
    "            d_head=config.d_model // config.n_heads,  # Adjust d_head accordingly\n",
    "            n_heads=config.n_heads,\n",
    "            d_mlp=config.d_model * 4,\n",
    "            normalization_type='LayerNorm',\n",
    "            d_vocab_out=config.d_model,\n",
    "            attention_dir='causal',\n",
    "            n_ctx=config.max_length\n",
    "\n",
    "        )\n",
    "        self.transformer = HookedTransformer(transformer_cfg)\n",
    "\n",
    "        # Output head for action prediction\n",
    "        self.action_head = nn.Linear(config.d_model, config.n_nodes)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Initialize weights as needed\n",
    "        pass\n",
    "\n",
    "    def forward(self, nodes, times, rewards, actions=None):\n",
    "        # Embeddings\n",
    "        node_embeddings = self.node_embedding(nodes)  # [batch_size, seq_length, d_model]\n",
    "        time_embeddings = self.time_embedding(times)  # [batch_size, seq_length, d_model]\n",
    "        reward_embeddings = self.reward_embedding(rewards.unsqueeze(-1))  # [batch_size, seq_length, d_model]\n",
    "\n",
    "        # Combine embeddings\n",
    "        embeddings = node_embeddings + time_embeddings + reward_embeddings\n",
    "\n",
    "        # Transformer\n",
    "        x = rearrange(embeddings, \"b s d -> s b d\")\n",
    "        transformer_output = self.transformer(x)\n",
    "        transformer_output = rearrange(transformer_output, \"s b d -> b s d\")\n",
    "\n",
    "        # Action prediction\n",
    "        action_logits = self.action_head(transformer_output)\n",
    "\n",
    "        return torch.log_softmax(action_logits, dim=-1)\n",
    "\n",
    "    # Additional methods for sampling, reward prediction, etc.\n",
    "\n",
    "# Configuration and model initialization\n",
    "config = RandomWalksTransformerConfig()\n",
    "model = DecisionTransformer(config)\n",
    "\n",
    "# Example data\n",
    "batch_size = 5\n",
    "seq_length = config.max_length\n",
    "nodes = torch.randint(0, config.n_nodes, (batch_size, seq_length))  # Random sequence of nodes\n",
    "times = torch.arange(seq_length).expand(batch_size, seq_length)  # Position sequence\n",
    "rewards = torch.randn(batch_size, seq_length)  # Example rewards\n",
    "\n",
    "# Forward pass\n",
    "logits = model(nodes, times, rewards)\n",
    "predicted_actions = torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTransformer:\n",
    "\n",
    "    def __init__(self, transformer_config):\n",
    "        self.transformer_config = transformer_config\n",
    "\n",
    "        # Why is this in a sequential? \n",
    "        self.action_embedding = nn.Sequential(\n",
    "            nn.Embedding(\n",
    "                environment_config.action_space.n + 1,\n",
    "                self.transformer_config.d_model,\n",
    "            )\n",
    "        )\n",
    "        self.time_embedding = self.initialize_time_embedding()\n",
    "        self.state_embedding = self.initialize_state_embedding()\n",
    "\n",
    "        # Initialise weights\n",
    "        nn.init.normal_(\n",
    "            self.action_embedding[0].weight,\n",
    "            mean=0.0,\n",
    "            std=1\n",
    "            / (\n",
    "                (environment_config.action_space.n + 1 + 1)\n",
    "                * self.transformer_config.d_model\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.transformer = self.initialize_easy_transformer()\n",
    "\n",
    "        self.action_predictor = nn.Linear(\n",
    "            self.transformer_config.d_model, environment_config.action_space.n\n",
    "        )\n",
    "        self.initialize_state_predictor()\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "        self.reward_embedding = nn.Sequential(\n",
    "            nn.Linear(1, self.transformer_config.d_model, bias=False)\n",
    "        )\n",
    "        self.reward_predictor = nn.Linear(self.transformer_config.d_model, 1)\n",
    "\n",
    "        # n_ctx include full timesteps except for the last where it doesn't know the action\n",
    "        assert (transformer_config.n_ctx - 2) % 3 == 0\n",
    "\n",
    "        self.initialize_weights()\n",
    "        \n",
    "\n",
    "    def self.initialise_easy_transformer():\n",
    "\n",
    "        # Transformer from Neel Nanda's transformer-lens library\n",
    "        cfg = HookedTransformerConfig(\n",
    "            n_layers=self.transformer_config.n_layers,\n",
    "            d_model=self.transformer_config.d_model,\n",
    "            d_head=self.transformer_config.d_head,\n",
    "            n_heads=self.transformer_config.n_heads,\n",
    "            d_mlp=self.transformer_config.d_mlp,\n",
    "            d_vocab=self.transformer_config.d_model,\n",
    "            # 3x the max timestep so we have room for an action, reward, and state per timestep\n",
    "            n_ctx=self.transformer_config.n_ctx,\n",
    "            act_fn=self.transformer_config.activation_fn,\n",
    "            gated_mlp=self.transformer_config.gated_mlp,\n",
    "            normalization_type=self.transformer_config.layer_norm,\n",
    "            attention_dir=\"causal\",\n",
    "            d_vocab_out=self.transformer_config.d_model,\n",
    "            seed=self.transformer_config.seed,\n",
    "            device=self.transformer_config.device,\n",
    "        )\n",
    "\n",
    "        assert (cfg.attention_dir == \"causal\"), \"Attention direction must be causal\"\n",
    "\n",
    "        transformer = HookedTransformer(cfg)\n",
    "\n",
    "        # Because we passing in tokens, turn off embedding and update the position embedding\n",
    "        transformer.embed = nn.Identity()\n",
    "        transformer.pos_embed = PosEmbedTokens(cfg)\n",
    "        # initialise position embedding\n",
    "        nn.init.normal_(transformer.pos_embed.W_pos, cfg.initializer_range)\n",
    "        # don't unembed, we'll do that ourselves.\n",
    "        transformer.unembed = nn.Identity()\n",
    "\n",
    "        return transformer\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        \"\"\"\n",
    "        TransformerLens is weird so we have to use the module path and can't just rely on the module \n",
    "        instance as we do would be the default approach in pytorch.\n",
    "        \"\"\"\n",
    "        self.apply(self._init_weights_classic)\n",
    "\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"W_\" in name: nn.init.normal_(param, std=0.02)\n",
    "\n",
    "    def _init_weights_classic(self, module):\n",
    "        \"\"\"\n",
    "        Use Min GPT Method.\n",
    "        https://github.com/karpathy/minGPT/blob/37baab71b9abea1b76ab957409a1cc2fbfba8a26/mingpt/model.py#L163\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "        elif (\n",
    "            \"PosEmbedTokens\" in module._get_name()\n",
    "        ):  # transformer lens components\n",
    "            for param in module.parameters():\n",
    "                torch.nn.init.normal_(param, mean=0.0, std=0.02)\n",
    "    \n",
    "    def get_time_embedding(self, timesteps):\n",
    "        assert (timesteps.max() <= self.environment_config.max_steps), \"timesteps must be less than max_timesteps\"\n",
    "\n",
    "        block_size = timesteps.shape[1]\n",
    "        timesteps = rearrange(timesteps, \"batch block time-> (batch block) time\")\n",
    "        time_embeddings = self.time_embedding(timesteps)\n",
    "        if self.transformer_config.time_embedding_type != \"linear\": time_embeddings = time_embeddings.squeeze(-2)\n",
    "        time_embeddings = rearrange(\n",
    "            time_embeddings,\n",
    "            \"(batch block) n_embd -> batch block n_embd\",\n",
    "            block=block_size,\n",
    "        )\n",
    "        return time_embeddings\n",
    "\n",
    "    def get_state_embedding(self, states):\n",
    "        # embed states and recast back to (batch, block_size, n_embd)\n",
    "        block_size = states.shape[1]\n",
    "        if self.transformer_config.state_embedding_type.lower() in [\n",
    "            \"cnn\",\n",
    "            \"vit\",\n",
    "        ]:\n",
    "            states = rearrange(\n",
    "                states,\n",
    "                \"batch block height width channel -> (batch block) height width channel\",\n",
    "            )\n",
    "            state_embeddings = self.state_embedding(\n",
    "                states.type(torch.float32).contiguous()\n",
    "            )  # (batch * block_size, n_embd)\n",
    "\n",
    "        elif self.transformer_config.state_embedding_type.lower() == \"grid\":\n",
    "            states = rearrange(\n",
    "                states,\n",
    "                \"batch block height width channel -> (batch block) (channel height width)\",\n",
    "            )\n",
    "            state_embeddings = self.state_embedding(\n",
    "                states.type(torch.float32).contiguous()\n",
    "            )  # (batch * block_size, n_embd)\n",
    "        else:\n",
    "            states = rearrange(\n",
    "                states, \"batch block state_dim -> (batch block) state_dim\"\n",
    "            )\n",
    "            state_embeddings = self.state_embedding(\n",
    "                states.type(torch.float32).contiguous()\n",
    "            )\n",
    "        state_embeddings = rearrange(\n",
    "            state_embeddings,\n",
    "            \"(batch block) n_embd -> batch block n_embd\",\n",
    "            block=block_size,\n",
    "        )\n",
    "        return state_embeddings\n",
    "\n",
    "    def get_action_embedding(self, actions):\n",
    "        block_size = actions.shape[1]\n",
    "        if block_size == 0:\n",
    "            return None  # no actions to embed\n",
    "        actions = rearrange(\n",
    "            actions, \"batch block action -> (batch block) action\"\n",
    "        )\n",
    "        # I don't see why we need this but we do? Maybe because of the sequential?\n",
    "        action_embeddings = self.action_embedding(actions).flatten(1)\n",
    "        action_embeddings = rearrange(\n",
    "            action_embeddings,\n",
    "            \"(batch block) n_embd -> batch block n_embd\",\n",
    "            block=block_size,\n",
    "        )\n",
    "        return action_embeddings\n",
    "\n",
    "    def initialize_time_embedding(self):\n",
    "        if not (self.transformer_config.time_embedding_type == \"linear\"):\n",
    "            self.time_embedding = nn.Embedding(\n",
    "                self.environment_config.max_steps + 1,\n",
    "                self.transformer_config.d_model,\n",
    "            )\n",
    "        else:\n",
    "            self.time_embedding = nn.Linear(1, self.transformer_config.d_model)\n",
    "\n",
    "        return self.time_embedding\n",
    "\n",
    "    def initialize_state_embedding(self):\n",
    "        if self.transformer_config.state_embedding_type.lower() == \"cnn\":\n",
    "            state_embedding = MiniGridConvEmbedder(\n",
    "                self.transformer_config.d_model, endpool=True\n",
    "            )\n",
    "        elif self.transformer_config.state_embedding_type.lower() == \"vit\":\n",
    "            state_embedding = MiniGridViTEmbedder(\n",
    "                self.transformer_config.d_model,\n",
    "            )\n",
    "        else:\n",
    "            if isinstance(self.environment_config.observation_space, Dict):\n",
    "                n_obs = np.prod(\n",
    "                    self.environment_config.observation_space[\"image\"].shape\n",
    "                )\n",
    "            else:\n",
    "                n_obs = np.prod(\n",
    "                    self.environment_config.observation_space.shape\n",
    "                )\n",
    "\n",
    "            state_embedding = nn.Linear(\n",
    "                n_obs, self.transformer_config.d_model, bias=False\n",
    "            )\n",
    "\n",
    "            nn.init.normal_(state_embedding.weight, mean=0.0, std=0.02)\n",
    "\n",
    "        return state_embedding\n",
    "\n",
    "    def initialize_state_predictor(self):\n",
    "        if isinstance(self.environment_config.observation_space, Box):\n",
    "            self.state_predictor = nn.Linear(\n",
    "                self.transformer_config.d_model,\n",
    "                np.prod(self.environment_config.observation_space.shape),\n",
    "            )\n",
    "        elif isinstance(self.environment_config.observation_space, Dict):\n",
    "            self.state_predictor = nn.Linear(\n",
    "                self.transformer_config.d_model,\n",
    "                np.prod(\n",
    "                    self.environment_config.observation_space[\"image\"].shape\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def predict_states(self, x):\n",
    "        return self.state_predictor(x)\n",
    "\n",
    "    def predict_actions(self, x):\n",
    "        return self.action_predictor(x)\n",
    "\n",
    "    def get_token_embeddings(\n",
    "        self,\n",
    "        state_embeddings,\n",
    "        time_embeddings,\n",
    "        reward_embeddings,\n",
    "        action_embeddings=None,\n",
    "        targets=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        We need to compose the embeddings for:\n",
    "            - states\n",
    "            - actions\n",
    "            - rewards\n",
    "            - time\n",
    "\n",
    "        Handling the cases where:\n",
    "        1. we are training:\n",
    "            1. we may not have action yet (reward, state)\n",
    "            2. we have (action, state, reward)...\n",
    "        2. we are evaluating:\n",
    "            1. we have a target \"a reward\" followed by state\n",
    "\n",
    "        1.1 and 2.1 are the same, but we need to handle the target as the initial reward.\n",
    "\n",
    "        \"\"\"\n",
    "        batches = state_embeddings.shape[0]\n",
    "        timesteps = time_embeddings.shape[1]\n",
    "\n",
    "        reward_embeddings = reward_embeddings + time_embeddings\n",
    "        state_embeddings = state_embeddings + time_embeddings\n",
    "\n",
    "        if action_embeddings is not None:\n",
    "            if action_embeddings.shape[1] < timesteps:\n",
    "                assert (\n",
    "                    action_embeddings.shape[1] == timesteps - 1\n",
    "                ), \"Action embeddings must be one timestep less than state embeddings\"\n",
    "                action_embeddings = (\n",
    "                    action_embeddings\n",
    "                    + time_embeddings[:, : action_embeddings.shape[1]]\n",
    "                )\n",
    "                trajectory_length = timesteps * 3 - 1\n",
    "            else:\n",
    "                action_embeddings = action_embeddings + time_embeddings\n",
    "                trajectory_length = timesteps * 3\n",
    "        else:\n",
    "            trajectory_length = 2  # one timestep, no action yet\n",
    "\n",
    "        if targets:\n",
    "            targets = targets + time_embeddings\n",
    "\n",
    "        # create the token embeddings\n",
    "        token_embeddings = torch.zeros(\n",
    "            (batches, trajectory_length, self.transformer_config.d_model),\n",
    "            dtype=torch.float32,\n",
    "            device=state_embeddings.device,\n",
    "        )  # batches, blocksize, n_embd\n",
    "\n",
    "        if action_embeddings is not None:\n",
    "            token_embeddings[:, ::3, :] = reward_embeddings\n",
    "            token_embeddings[:, 1::3, :] = state_embeddings\n",
    "            token_embeddings[:, 2::3, :] = action_embeddings\n",
    "        else:\n",
    "            token_embeddings[:, 0, :] = reward_embeddings[:, 0, :]\n",
    "            token_embeddings[:, 1, :] = state_embeddings[:, 0, :]\n",
    "\n",
    "        if targets is not None:\n",
    "            target_embedding = self.reward_embedding(targets)\n",
    "            token_embeddings[:, 0, :] = target_embedding[:, 0, :]\n",
    "\n",
    "        return token_embeddings\n",
    "\n",
    "    def to_tokens(self, states, actions, rtgs, timesteps):\n",
    "        # embed states and recast back to (batch, block_size, n_embd)\n",
    "        state_embeddings = self.get_state_embedding(\n",
    "            states\n",
    "        )  # batch_size, block_size, n_embd\n",
    "        action_embeddings = (\n",
    "            self.get_action_embedding(actions) if actions is not None else None\n",
    "        )  # batch_size, block_size, n_embd or None\n",
    "        reward_embeddings = self.get_reward_embedding(\n",
    "            rtgs\n",
    "        )  # batch_size, block_size, n_embd\n",
    "        time_embeddings = self.get_time_embedding(\n",
    "            timesteps\n",
    "        )  # batch_size, block_size, n_embd\n",
    "\n",
    "        # use state_embeddings, actions, rewards to go and\n",
    "        token_embeddings = self.get_token_embeddings(\n",
    "            state_embeddings=state_embeddings,\n",
    "            action_embeddings=action_embeddings,\n",
    "            reward_embeddings=reward_embeddings,\n",
    "            time_embeddings=time_embeddings,\n",
    "        )\n",
    "        return token_embeddings\n",
    "\n",
    "    def get_action(self, states, actions, rewards, timesteps):\n",
    "        state_preds, action_preds, reward_preds = self.forward(\n",
    "            states, actions, rewards, timesteps\n",
    "        )\n",
    "\n",
    "        # get the action prediction\n",
    "        action_preds = action_preds[:, -1, :]  # (batch, n_actions)\n",
    "        action = torch.argmax(action_preds, dim=-1)  # (batch)\n",
    "        return action\n",
    "\n",
    "    def get_reward_embedding(self, rtgs):\n",
    "        block_size = rtgs.shape[1]\n",
    "        rtgs = rearrange(rtgs, \"batch block rtg -> (batch block) rtg\")\n",
    "        rtg_embeddings = self.reward_embedding(rtgs.type(torch.float32))\n",
    "        rtg_embeddings = rearrange(\n",
    "            rtg_embeddings,\n",
    "            \"(batch block) n_embd -> batch block n_embd\",\n",
    "            block=block_size,\n",
    "        )\n",
    "        return rtg_embeddings\n",
    "\n",
    "    def get_logits(self, x, batch_size, seq_length, no_actions: bool):\n",
    "        if no_actions is False:\n",
    "            # TODO replace with einsum\n",
    "            if (x.shape[1] % 3 != 0) and ((x.shape[1] + 1) % 3 == 0):\n",
    "                x = torch.concat((x, x[:, -2].unsqueeze(1)), dim=1)\n",
    "\n",
    "            x = x.reshape(\n",
    "                batch_size, seq_length, 3, self.transformer_config.d_model\n",
    "            )\n",
    "            x = x.permute(0, 2, 1, 3)\n",
    "\n",
    "            # predict next return given state and action\n",
    "            reward_preds = self.predict_rewards(x[:, 2])\n",
    "            # predict next state given state and action\n",
    "            state_preds = self.predict_states(x[:, 2])\n",
    "            # predict next action given state and RTG\n",
    "            action_preds = self.predict_actions(x[:, 1])\n",
    "            return state_preds, action_preds, reward_preds\n",
    "\n",
    "        else:\n",
    "            # TODO replace with einsum\n",
    "            x = x.reshape(\n",
    "                batch_size, seq_length, 2, self.transformer_config.d_model\n",
    "            )\n",
    "            x = x.permute(0, 2, 1, 3)\n",
    "            # predict next action given state and RTG\n",
    "            action_preds = self.predict_actions(x[:, 1])\n",
    "            return None, action_preds, None\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        # has variable shape, starting with batch, position\n",
    "        states: TT[...],  # noqa: F821\n",
    "        actions: TT[\"batch\", \"position\"],  # noqa: F821\n",
    "        rtgs: TT[\"batch\", \"position\"],  # noqa: F821\n",
    "        timesteps: TT[\"batch\", \"position\"],  # noqa: F821\n",
    "        pad_action: bool = True,\n",
    "    ) -> Tuple[\n",
    "        TT[...], TT[\"batch\", \"position\"], TT[\"batch\", \"position\"]  # noqa: F821\n",
    "    ]:\n",
    "        batch_size = states.shape[0]\n",
    "        seq_length = states.shape[1]\n",
    "        no_actions = actions is None\n",
    "\n",
    "        if no_actions is False:\n",
    "            if actions.shape[1] < seq_length - 1:\n",
    "                raise ValueError(\n",
    "                    f\"Actions required for all timesteps except the last, got {actions.shape[1]} and {seq_length}\"\n",
    "                )\n",
    "\n",
    "        # embed states and recast back to (batch, block_size, n_embd)\n",
    "        token_embeddings = self.to_tokens(states, actions, rtgs, timesteps)\n",
    "        x = self.transformer(token_embeddings)\n",
    "        state_preds, action_preds, reward_preds = self.get_logits(\n",
    "            x, batch_size, seq_length, no_actions=no_actions\n",
    "        )\n",
    "\n",
    "        return state_preds, action_preds, reward_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "from gymnasium.spaces import Box, Dict\n",
    "from torchtyping import TensorType as TT\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "from dataclasses import dataclasss\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TransformerModelConfig:\n",
    "\n",
    "    d_model: int = 128\n",
    "    n_heads: int = 4\n",
    "    d_mlp: int = 256\n",
    "    n_layers: int = 2\n",
    "    n_ctx: int = 2\n",
    "    layer_norm: Optional[str] = None\n",
    "    gated_mlp: bool = False\n",
    "    activation_fn: str = \"relu\"\n",
    "    state_embedding_type: str = \"grid\"\n",
    "    time_embedding_type: str = \"embedding\"\n",
    "    seed: int = 1\n",
    "    device: str = \"cpu\"\n",
    "\n",
    "\n",
    "class TrajectoryTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Base Class for trajectory modelling transformers including:\n",
    "        - Decision Transformer (offline, RTG, (R,s,a))\n",
    "        - Online Transformer (online, reward, (s,a,r) or (s,a))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transformer_config: TransformerModelConfig,\n",
    "        environment_config: EnvironmentConfig,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformer_config = transformer_config\n",
    "        self.environment_config = environment_config\n",
    "\n",
    "        # Why is this in a sequential? Need to get rid of it at some\n",
    "        # point when I don't care about loading older models.\n",
    "        self.action_embedding = nn.Sequential(\n",
    "            nn.Embedding(\n",
    "                environment_config.action_space.n + 1,\n",
    "                self.transformer_config.d_model,\n",
    "            )\n",
    "        )\n",
    "        self.time_embedding = self.initialize_time_embedding()\n",
    "        self.state_embedding = self.initialize_state_embedding()\n",
    "\n",
    "        # Initialize weights\n",
    "        nn.init.normal_(\n",
    "            self.action_embedding[0].weight,\n",
    "            mean=0.0,\n",
    "            std=1\n",
    "            / (\n",
    "                (environment_config.action_space.n + 1 + 1)\n",
    "                * self.transformer_config.d_model\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.transformer = self.initialize_easy_transformer()\n",
    "\n",
    "        self.action_predictor = nn.Linear(\n",
    "            self.transformer_config.d_model, environment_config.action_space.n\n",
    "        )\n",
    "        self.initialize_state_predictor()\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        \"\"\"\n",
    "        TransformerLens is weird so we have to use the module path and can't just rely on the module \n",
    "        instance as we do would be the default approach in pytorch.\n",
    "        \"\"\"\n",
    "        self.apply(self._init_weights_classic)\n",
    "\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"W_\" in name: nn.init.normal_(param, std=0.02)\n",
    "\n",
    "    def _init_weights_classic(self, module):\n",
    "        \"\"\"\n",
    "        Use Min GPT Method.\n",
    "        https://github.com/karpathy/minGPT/blob/37baab71b9abea1b76ab957409a1cc2fbfba8a26/mingpt/model.py#L163\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "        elif (\n",
    "            \"PosEmbedTokens\" in module._get_name()\n",
    "        ):  # transformer lens components\n",
    "            for param in module.parameters():\n",
    "                torch.nn.init.normal_(param, mean=0.0, std=0.02)\n",
    "\n",
    "    def get_time_embedding(self, timesteps):\n",
    "        assert (\n",
    "            timesteps.max() <= self.environment_config.max_steps\n",
    "        ), \"timesteps must be less than max_timesteps\"\n",
    "\n",
    "        block_size = timesteps.shape[1]\n",
    "        timesteps = rearrange(\n",
    "            timesteps, \"batch block time-> (batch block) time\"\n",
    "        )\n",
    "        time_embeddings = self.time_embedding(timesteps)\n",
    "        if self.transformer_config.time_embedding_type != \"linear\":\n",
    "            time_embeddings = time_embeddings.squeeze(-2)\n",
    "        time_embeddings = rearrange(\n",
    "            time_embeddings,\n",
    "            \"(batch block) n_embd -> batch block n_embd\",\n",
    "            block=block_size,\n",
    "        )\n",
    "        return time_embeddings\n",
    "\n",
    "    def get_state_embedding(self, states):\n",
    "        # embed states and recast back to (batch, block_size, n_embd)\n",
    "        block_size = states.shape[1]\n",
    "        if self.transformer_config.state_embedding_type.lower() in [\n",
    "            \"cnn\",\n",
    "            \"vit\",\n",
    "        ]:\n",
    "            states = rearrange(\n",
    "                states,\n",
    "                \"batch block height width channel -> (batch block) height width channel\",\n",
    "            )\n",
    "            state_embeddings = self.state_embedding(\n",
    "                states.type(torch.float32).contiguous()\n",
    "            )  # (batch * block_size, n_embd)\n",
    "\n",
    "        elif self.transformer_config.state_embedding_type.lower() == \"grid\":\n",
    "            states = rearrange(\n",
    "                states,\n",
    "                \"batch block height width channel -> (batch block) (channel height width)\",\n",
    "            )\n",
    "            state_embeddings = self.state_embedding(\n",
    "                states.type(torch.float32).contiguous()\n",
    "            )  # (batch * block_size, n_embd)\n",
    "        else:\n",
    "            states = rearrange(\n",
    "                states, \"batch block state_dim -> (batch block) state_dim\"\n",
    "            )\n",
    "            state_embeddings = self.state_embedding(\n",
    "                states.type(torch.float32).contiguous()\n",
    "            )\n",
    "        state_embeddings = rearrange(\n",
    "            state_embeddings,\n",
    "            \"(batch block) n_embd -> batch block n_embd\",\n",
    "            block=block_size,\n",
    "        )\n",
    "        return state_embeddings\n",
    "\n",
    "    def get_action_embedding(self, actions):\n",
    "        block_size = actions.shape[1]\n",
    "        if block_size == 0:\n",
    "            return None  # no actions to embed\n",
    "        actions = rearrange(\n",
    "            actions, \"batch block action -> (batch block) action\"\n",
    "        )\n",
    "        # I don't see why we need this but we do? Maybe because of the sequential?\n",
    "        action_embeddings = self.action_embedding(actions).flatten(1)\n",
    "        action_embeddings = rearrange(\n",
    "            action_embeddings,\n",
    "            \"(batch block) n_embd -> batch block n_embd\",\n",
    "            block=block_size,\n",
    "        )\n",
    "        return action_embeddings\n",
    "\n",
    "    def predict_states(self, x):\n",
    "        return self.state_predictor(x)\n",
    "\n",
    "    def predict_actions(self, x):\n",
    "        return self.action_predictor(x)\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_token_embeddings(\n",
    "        self, state_embeddings, time_embeddings, action_embeddings, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Returns the token embeddings for the transformer input.\n",
    "        Note that different subclasses will have different token embeddings\n",
    "        such as the DecisionTransformer which will use RTG (placed before the\n",
    "        state embedding).\n",
    "\n",
    "        Args:\n",
    "            states: (batch, position, state_dim)\n",
    "            actions: (batch, position)\n",
    "            timesteps: (batch, position)\n",
    "        Kwargs:\n",
    "            rtgs: (batch, position) (only for DecisionTransformer)\n",
    "\n",
    "        Returns:\n",
    "            token_embeddings: (batch, position, n_embd)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_action(self, **kwargs) -> int:\n",
    "        \"\"\"\n",
    "        Returns the action given the state.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def initialize_time_embedding(self):\n",
    "        if not (self.transformer_config.time_embedding_type == \"linear\"):\n",
    "            self.time_embedding = nn.Embedding(\n",
    "                self.environment_config.max_steps + 1,\n",
    "                self.transformer_config.d_model,\n",
    "            )\n",
    "        else:\n",
    "            self.time_embedding = nn.Linear(1, self.transformer_config.d_model)\n",
    "\n",
    "        return self.time_embedding\n",
    "\n",
    "    def initialize_state_embedding(self):\n",
    "        if self.transformer_config.state_embedding_type.lower() == \"cnn\":\n",
    "            state_embedding = MiniGridConvEmbedder(\n",
    "                self.transformer_config.d_model, endpool=True\n",
    "            )\n",
    "        elif self.transformer_config.state_embedding_type.lower() == \"vit\":\n",
    "            state_embedding = MiniGridViTEmbedder(\n",
    "                self.transformer_config.d_model,\n",
    "            )\n",
    "        else:\n",
    "            if isinstance(self.environment_config.observation_space, Dict):\n",
    "                n_obs = np.prod(\n",
    "                    self.environment_config.observation_space[\"image\"].shape\n",
    "                )\n",
    "            else:\n",
    "                n_obs = np.prod(\n",
    "                    self.environment_config.observation_space.shape\n",
    "                )\n",
    "\n",
    "            state_embedding = nn.Linear(\n",
    "                n_obs, self.transformer_config.d_model, bias=False\n",
    "            )\n",
    "\n",
    "            nn.init.normal_(state_embedding.weight, mean=0.0, std=0.02)\n",
    "\n",
    "        return state_embedding\n",
    "\n",
    "    def initialize_state_predictor(self):\n",
    "        if isinstance(self.environment_config.observation_space, Box):\n",
    "            self.state_predictor = nn.Linear(\n",
    "                self.transformer_config.d_model,\n",
    "                np.prod(self.environment_config.observation_space.shape),\n",
    "            )\n",
    "        elif isinstance(self.environment_config.observation_space, Dict):\n",
    "            self.state_predictor = nn.Linear(\n",
    "                self.transformer_config.d_model,\n",
    "                np.prod(\n",
    "                    self.environment_config.observation_space[\"image\"].shape\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def initialize_easy_transformer(self):\n",
    "        # Transformer\n",
    "        cfg = HookedTransformerConfig(\n",
    "            n_layers=self.transformer_config.n_layers,\n",
    "            d_model=self.transformer_config.d_model,\n",
    "            d_head=self.transformer_config.d_head,\n",
    "            n_heads=self.transformer_config.n_heads,\n",
    "            d_mlp=self.transformer_config.d_mlp,\n",
    "            d_vocab=self.transformer_config.d_model,\n",
    "            # 3x the max timestep so we have room for an action, reward, and state per timestep\n",
    "            n_ctx=self.transformer_config.n_ctx,\n",
    "            act_fn=self.transformer_config.activation_fn,\n",
    "            gated_mlp=self.transformer_config.gated_mlp,\n",
    "            normalization_type=self.transformer_config.layer_norm,\n",
    "            attention_dir=\"causal\",\n",
    "            d_vocab_out=self.transformer_config.d_model,\n",
    "            seed=self.transformer_config.seed,\n",
    "            device=self.transformer_config.device,\n",
    "        )\n",
    "\n",
    "        assert (cfg.attention_dir == \"causal\"), \"Attention direction must be causal\"\n",
    "\n",
    "        transformer = HookedTransformer(cfg)\n",
    "\n",
    "        # Because we passing in tokens, turn off embedding and update the position embedding\n",
    "        transformer.embed = nn.Identity()\n",
    "        transformer.pos_embed = PosEmbedTokens(cfg)\n",
    "        # initialize position embedding\n",
    "        nn.init.normal_(transformer.pos_embed.W_pos, cfg.initializer_range)\n",
    "        # don't unembed, we'll do that ourselves.\n",
    "        transformer.unembed = nn.Identity()\n",
    "\n",
    "        return transformer\n",
    "\n",
    "\n",
    "class DecisionTransformer(TrajectoryTransformer):\n",
    "    def __init__(self, environment_config, transformer_config, **kwargs):\n",
    "        super().__init__(\n",
    "            environment_config=environment_config,\n",
    "            transformer_config=transformer_config,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.model_type = \"decision_transformer\"\n",
    "        self.reward_embedding = nn.Sequential(\n",
    "            nn.Linear(1, self.transformer_config.d_model, bias=False)\n",
    "        )\n",
    "        self.reward_predictor = nn.Linear(self.transformer_config.d_model, 1)\n",
    "\n",
    "        # n_ctx include full timesteps except for the last where it doesn't know the action\n",
    "        assert (transformer_config.n_ctx - 2) % 3 == 0\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def predict_rewards(self, x):\n",
    "        return self.reward_predictor(x)\n",
    "\n",
    "    def get_token_embeddings(\n",
    "        self,\n",
    "        state_embeddings,\n",
    "        time_embeddings,\n",
    "        reward_embeddings,\n",
    "        action_embeddings=None,\n",
    "        targets=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        We need to compose the embeddings for:\n",
    "            - states\n",
    "            - actions\n",
    "            - rewards\n",
    "            - time\n",
    "\n",
    "        Handling the cases where:\n",
    "        1. we are training:\n",
    "            1. we may not have action yet (reward, state)\n",
    "            2. we have (action, state, reward)...\n",
    "        2. we are evaluating:\n",
    "            1. we have a target \"a reward\" followed by state\n",
    "\n",
    "        1.1 and 2.1 are the same, but we need to handle the target as the initial reward.\n",
    "\n",
    "        \"\"\"\n",
    "        batches = state_embeddings.shape[0]\n",
    "        timesteps = time_embeddings.shape[1]\n",
    "\n",
    "        reward_embeddings = reward_embeddings + time_embeddings\n",
    "        state_embeddings = state_embeddings + time_embeddings\n",
    "\n",
    "        if action_embeddings is not None:\n",
    "            if action_embeddings.shape[1] < timesteps:\n",
    "                assert (\n",
    "                    action_embeddings.shape[1] == timesteps - 1\n",
    "                ), \"Action embeddings must be one timestep less than state embeddings\"\n",
    "                action_embeddings = (\n",
    "                    action_embeddings\n",
    "                    + time_embeddings[:, : action_embeddings.shape[1]]\n",
    "                )\n",
    "                trajectory_length = timesteps * 3 - 1\n",
    "            else:\n",
    "                action_embeddings = action_embeddings + time_embeddings\n",
    "                trajectory_length = timesteps * 3\n",
    "        else:\n",
    "            trajectory_length = 2  # one timestep, no action yet\n",
    "\n",
    "        if targets:\n",
    "            targets = targets + time_embeddings\n",
    "\n",
    "        # create the token embeddings\n",
    "        token_embeddings = torch.zeros(\n",
    "            (batches, trajectory_length, self.transformer_config.d_model),\n",
    "            dtype=torch.float32,\n",
    "            device=state_embeddings.device,\n",
    "        )  # batches, blocksize, n_embd\n",
    "\n",
    "        if action_embeddings is not None:\n",
    "            token_embeddings[:, ::3, :] = reward_embeddings\n",
    "            token_embeddings[:, 1::3, :] = state_embeddings\n",
    "            token_embeddings[:, 2::3, :] = action_embeddings\n",
    "        else:\n",
    "            token_embeddings[:, 0, :] = reward_embeddings[:, 0, :]\n",
    "            token_embeddings[:, 1, :] = state_embeddings[:, 0, :]\n",
    "\n",
    "        if targets is not None:\n",
    "            target_embedding = self.reward_embedding(targets)\n",
    "            token_embeddings[:, 0, :] = target_embedding[:, 0, :]\n",
    "\n",
    "        return token_embeddings\n",
    "\n",
    "    def to_tokens(self, states, actions, rtgs, timesteps):\n",
    "        # embed states and recast back to (batch, block_size, n_embd)\n",
    "        state_embeddings = self.get_state_embedding(\n",
    "            states\n",
    "        )  # batch_size, block_size, n_embd\n",
    "        action_embeddings = (\n",
    "            self.get_action_embedding(actions) if actions is not None else None\n",
    "        )  # batch_size, block_size, n_embd or None\n",
    "        reward_embeddings = self.get_reward_embedding(\n",
    "            rtgs\n",
    "        )  # batch_size, block_size, n_embd\n",
    "        time_embeddings = self.get_time_embedding(\n",
    "            timesteps\n",
    "        )  # batch_size, block_size, n_embd\n",
    "\n",
    "        # use state_embeddings, actions, rewards to go and\n",
    "        token_embeddings = self.get_token_embeddings(\n",
    "            state_embeddings=state_embeddings,\n",
    "            action_embeddings=action_embeddings,\n",
    "            reward_embeddings=reward_embeddings,\n",
    "            time_embeddings=time_embeddings,\n",
    "        )\n",
    "        return token_embeddings\n",
    "\n",
    "    def get_action(self, states, actions, rewards, timesteps):\n",
    "        state_preds, action_preds, reward_preds = self.forward(\n",
    "            states, actions, rewards, timesteps\n",
    "        )\n",
    "\n",
    "        # get the action prediction\n",
    "        action_preds = action_preds[:, -1, :]  # (batch, n_actions)\n",
    "        action = torch.argmax(action_preds, dim=-1)  # (batch)\n",
    "        return action\n",
    "\n",
    "    def get_reward_embedding(self, rtgs):\n",
    "        block_size = rtgs.shape[1]\n",
    "        rtgs = rearrange(rtgs, \"batch block rtg -> (batch block) rtg\")\n",
    "        rtg_embeddings = self.reward_embedding(rtgs.type(torch.float32))\n",
    "        rtg_embeddings = rearrange(\n",
    "            rtg_embeddings,\n",
    "            \"(batch block) n_embd -> batch block n_embd\",\n",
    "            block=block_size,\n",
    "        )\n",
    "        return rtg_embeddings\n",
    "\n",
    "    def get_logits(self, x, batch_size, seq_length, no_actions: bool):\n",
    "        if no_actions is False:\n",
    "            # TODO replace with einsum\n",
    "            if (x.shape[1] % 3 != 0) and ((x.shape[1] + 1) % 3 == 0):\n",
    "                x = torch.concat((x, x[:, -2].unsqueeze(1)), dim=1)\n",
    "\n",
    "            x = x.reshape(\n",
    "                batch_size, seq_length, 3, self.transformer_config.d_model\n",
    "            )\n",
    "            x = x.permute(0, 2, 1, 3)\n",
    "\n",
    "            # predict next return given state and action\n",
    "            reward_preds = self.predict_rewards(x[:, 2])\n",
    "            # predict next state given state and action\n",
    "            state_preds = self.predict_states(x[:, 2])\n",
    "            # predict next action given state and RTG\n",
    "            action_preds = self.predict_actions(x[:, 1])\n",
    "            return state_preds, action_preds, reward_preds\n",
    "\n",
    "        else:\n",
    "            # TODO replace with einsum\n",
    "            x = x.reshape(\n",
    "                batch_size, seq_length, 2, self.transformer_config.d_model\n",
    "            )\n",
    "            x = x.permute(0, 2, 1, 3)\n",
    "            # predict next action given state and RTG\n",
    "            action_preds = self.predict_actions(x[:, 1])\n",
    "            return None, action_preds, None\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        # has variable shape, starting with batch, position\n",
    "        states: TT[...],  # noqa: F821\n",
    "        actions: TT[\"batch\", \"position\"],  # noqa: F821\n",
    "        rtgs: TT[\"batch\", \"position\"],  # noqa: F821\n",
    "        timesteps: TT[\"batch\", \"position\"],  # noqa: F821\n",
    "        pad_action: bool = True,\n",
    "    ) -> Tuple[\n",
    "        TT[...], TT[\"batch\", \"position\"], TT[\"batch\", \"position\"]  # noqa: F821\n",
    "    ]:\n",
    "        batch_size = states.shape[0]\n",
    "        seq_length = states.shape[1]\n",
    "        no_actions = actions is None\n",
    "\n",
    "        if no_actions is False:\n",
    "            if actions.shape[1] < seq_length - 1:\n",
    "                raise ValueError(\n",
    "                    f\"Actions required for all timesteps except the last, got {actions.shape[1]} and {seq_length}\"\n",
    "                )\n",
    "\n",
    "        # embed states and recast back to (batch, block_size, n_embd)\n",
    "        token_embeddings = self.to_tokens(states, actions, rtgs, timesteps)\n",
    "        x = self.transformer(token_embeddings)\n",
    "        state_preds, action_preds, reward_preds = self.get_logits(\n",
    "            x, batch_size, seq_length, no_actions=no_actions\n",
    "        )\n",
    "\n",
    "        return state_preds, action_preds, reward_preds\n",
    "\n",
    "\n",
    "class CloneTransformer(TrajectoryTransformer):\n",
    "    \"\"\"\n",
    "    Behavioral clone modelling transformer including:\n",
    "        - CloneTransformer (offline, (s,a))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transformer_config: TransformerModelConfig,\n",
    "        environment_config: EnvironmentConfig,\n",
    "    ):\n",
    "        super().__init__(transformer_config, environment_config)\n",
    "        self.model_type = \"clone_transformer\"\n",
    "        # n_ctx must be odd (previous state, action, next state)\n",
    "        assert (transformer_config.n_ctx - 1) % 2 == 0\n",
    "        self.transformer = (\n",
    "            self.initialize_easy_transformer()\n",
    "        )  # this might not be needed?\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def get_token_embeddings(\n",
    "        self, state_embeddings, time_embeddings, action_embeddings=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Returns the token embeddings for the transformer input.\n",
    "        Note that different subclasses will have different token embeddings\n",
    "        such as the DecisionTransformer which will use RTG (placed before the\n",
    "        state embedding).\n",
    "\n",
    "        Args:\n",
    "            states: (batch, position, state_dim)\n",
    "            actions: (batch, position)\n",
    "\n",
    "        Returns:\n",
    "            token_embeddings: (batch, position, n_embd)\n",
    "        \"\"\"\n",
    "        batches = state_embeddings.shape[0]\n",
    "        timesteps = time_embeddings.shape[1]\n",
    "\n",
    "        state_embeddings = state_embeddings + time_embeddings\n",
    "\n",
    "        if action_embeddings is not None:\n",
    "            if action_embeddings.shape[1] == time_embeddings.shape[1] - 1:\n",
    "                # missing action for last t-step.\n",
    "                action_embeddings = action_embeddings + time_embeddings[:, :-1]\n",
    "                # repeat the last action embedding for the last timestep\n",
    "                action_embeddings = torch.cat(\n",
    "                    [\n",
    "                        action_embeddings,\n",
    "                        action_embeddings[:, -1, :].unsqueeze(1),\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "                # now the last action and second last are duplicates but we can fix this later. (TODO)\n",
    "                trajectory_length = timesteps * 2\n",
    "            else:\n",
    "                action_embeddings = action_embeddings + time_embeddings\n",
    "                trajectory_length = timesteps * 2\n",
    "        else:\n",
    "            trajectory_length = 1  # one timestep, no action yet\n",
    "\n",
    "        # create the token embeddings\n",
    "        token_embeddings = torch.zeros(\n",
    "            (batches, trajectory_length, self.transformer_config.d_model),\n",
    "            dtype=torch.float32,\n",
    "            device=state_embeddings.device,\n",
    "        )  # batches, blocksize, n_embd\n",
    "\n",
    "        if action_embeddings is not None:\n",
    "            token_embeddings[:, 0::2, :] = state_embeddings\n",
    "            token_embeddings[:, 1::2, :] = action_embeddings\n",
    "        else:\n",
    "            token_embeddings[:, 0, :] = state_embeddings[:, 0, :]\n",
    "\n",
    "        return token_embeddings\n",
    "\n",
    "    def to_tokens(self, states, actions, timesteps):\n",
    "        # embed states and recast back to (batch, block_size, n_embd)\n",
    "        state_embeddings = self.get_state_embedding(\n",
    "            states\n",
    "        )  # batch_size, block_size, n_embd\n",
    "        action_embeddings = (\n",
    "            self.get_action_embedding(actions) if actions is not None else None\n",
    "        )  # batch_size, block_size, n_embd or None\n",
    "        time_embeddings = self.get_time_embedding(\n",
    "            timesteps\n",
    "        )  # batch_size, block_size, n_embd\n",
    "\n",
    "        # use state_embeddings, actions, rewards to go and\n",
    "        token_embeddings = self.get_token_embeddings(\n",
    "            state_embeddings=state_embeddings,\n",
    "            action_embeddings=action_embeddings,\n",
    "            time_embeddings=time_embeddings,\n",
    "        )\n",
    "        return token_embeddings\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        # has variable shape, starting with batch, position\n",
    "        states: TT[...],\n",
    "        actions: TT[\"batch\", \"position\"],  # noqa: F821\n",
    "        timesteps: TT[\"batch\", \"position\"],  # noqa: F821\n",
    "        pad_action: bool = True,\n",
    "    ) -> Tuple[\n",
    "        TT[...], TT[\"batch\", \"position\"], TT[\"batch\", \"position\"]  # noqa: F821\n",
    "    ]:\n",
    "        batch_size = states.shape[0]\n",
    "        seq_length = states.shape[1]\n",
    "\n",
    "        if (\n",
    "            seq_length + (seq_length - 1) * (actions is not None)\n",
    "            > self.transformer_config.n_ctx\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                f\"Sequence length is too long for transformer, got {seq_length} and {self.transformer_config.n_ctx}\"\n",
    "            )\n",
    "\n",
    "        no_actions = (actions is None) or (actions.shape[1] == 0)\n",
    "\n",
    "        if no_actions is False:\n",
    "            if actions.shape[1] < seq_length - 1:\n",
    "                raise ValueError(\n",
    "                    f\"Actions required for all timesteps except the last, got {actions.shape[1]} and {seq_length}\"\n",
    "                )\n",
    "\n",
    "            if actions.shape[1] != seq_length - 1:\n",
    "                if pad_action:\n",
    "                    print(\n",
    "                        \"Warning: actions are missing for the last timestep, padding with zeros\"\n",
    "                    )\n",
    "                    # This means that you can't interpret Reward or State predictions for the last timestep!!!\n",
    "                    actions = torch.cat(\n",
    "                        [\n",
    "                            torch.zeros(\n",
    "                                batch_size,\n",
    "                                1,\n",
    "                                1,\n",
    "                                dtype=torch.long,\n",
    "                                device=actions.device,\n",
    "                            ),\n",
    "                            actions,\n",
    "                        ],\n",
    "                        dim=1,\n",
    "                    )\n",
    "\n",
    "        # embed states and recast back to (batch, block_size, n_embd)\n",
    "        token_embeddings = self.to_tokens(states, actions, timesteps)\n",
    "\n",
    "        if no_actions is False:\n",
    "            if actions.shape[1] == states.shape[1] - 1:\n",
    "                x = self.transformer(token_embeddings[:, :-1])\n",
    "                # concat last action embedding to the end of the transformer output x[:,-2].unsqueeze(1)\n",
    "                x = torch.cat(\n",
    "                    [x, token_embeddings[:, -2, :].unsqueeze(1)], dim=1\n",
    "                )\n",
    "                state_preds, action_preds = self.get_logits(\n",
    "                    x, batch_size, seq_length, no_actions=no_actions\n",
    "                )\n",
    "            else:\n",
    "                x = self.transformer(token_embeddings)\n",
    "                state_preds, action_preds = self.get_logits(\n",
    "                    x, batch_size, seq_length, no_actions=no_actions\n",
    "                )\n",
    "        else:\n",
    "            x = self.transformer(token_embeddings)\n",
    "            state_preds, action_preds = self.get_logits(\n",
    "                x, batch_size, seq_length, no_actions=no_actions\n",
    "            )\n",
    "\n",
    "        return state_preds, action_preds\n",
    "\n",
    "    def get_action(self, states, actions, timesteps):\n",
    "        state_preds, action_preds = self.forward(states, actions, timesteps)\n",
    "\n",
    "        # get the action prediction\n",
    "        action_preds = action_preds[:, -1, :]  # (batch, n_actions)\n",
    "        action = torch.argmax(action_preds, dim=-1)  # (batch)\n",
    "        return action\n",
    "\n",
    "    def get_logits(self, x, batch_size, seq_length, no_actions: bool):\n",
    "        # TODO replace with einsum\n",
    "        if not no_actions:\n",
    "            x = x.reshape(\n",
    "                batch_size, seq_length, 2, self.transformer_config.d_model\n",
    "            ).permute(0, 2, 1, 3)\n",
    "            # predict next return given state and action\n",
    "            # reward_preds = self.predict_rewards(x[:, 2])\n",
    "            # predict next state given state and action\n",
    "            state_preds = self.predict_states(x[:, 1])\n",
    "            # predict next action given state\n",
    "            action_preds = self.predict_actions(x[:, 0])\n",
    "\n",
    "            return state_preds, action_preds\n",
    "        else:\n",
    "            x = x.reshape(\n",
    "                batch_size, seq_length, 1, self.transformer_config.d_model\n",
    "            ).permute(0, 2, 1, 3)\n",
    "\n",
    "            # predict next return given state and action\n",
    "            # reward_preds = self.predict_rewards(x[:, 2])\n",
    "            # predict next state given state and action\n",
    "            # predict next action given state\n",
    "            action_preds = self.predict_actions(x[:, 0])\n",
    "\n",
    "            return None, action_preds\n",
    "\n",
    "\n",
    "class ActorTransformer(CloneTransformer):\n",
    "    \"\"\"\n",
    "    Identical to clone transformer but forward pass can only return action predictions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transformer_config: TransformerModelConfig,\n",
    "        environment_config: EnvironmentConfig,\n",
    "    ):\n",
    "        super().__init__(transformer_config, environment_config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        # has variable shape, starting with batch, position\n",
    "        states: TT[...],\n",
    "        actions: TT[\"batch\", \"position\"],  # noqa: F821\n",
    "        timesteps: TT[\"batch\", \"position\"],  # noqa: F821\n",
    "        pad_action: bool = True,\n",
    "    ) -> TT[\"batch\", \"position\"]:  # noqa: F821\n",
    "        _, action_preds = super().forward(\n",
    "            states, actions, timesteps, pad_action=pad_action\n",
    "        )\n",
    "\n",
    "        return action_preds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic transformer on just the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 walks of which 29% arrived at destination\n",
      "Tokens shape = torch.Size([1, 10])\n",
      "Attn shape = torch.Size([1, 10])\n",
      "Actions shape = torch.Size([1, 9, 1])\n",
      "Rewards shape = torch.Size([1, 9])\n",
      "\n",
      "Tokens = tensor([[ 3, 18,  2, 10,  4,  6,  3, 18,  2, 10]])\n",
      "Attn = tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Actions = tensor([[[18],\n",
      "         [ 2],\n",
      "         [10],\n",
      "         [ 4],\n",
      "         [ 6],\n",
      "         [ 3],\n",
      "         [18],\n",
      "         [ 2],\n",
      "         [10]]])\n",
      "Rewards = tensor([[-100., -100., -100., -100., -100., -100., -100., -100., -100.]])\n"
     ]
    }
   ],
   "source": [
    "data = RandomWalks(seed=2)\n",
    "dataset = data.dataset\n",
    "dataloader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "# Print an element from the dataloader\n",
    "for batch in dataloader:\n",
    "    tokens, attn, rewards = batch\n",
    "    actions = tokens[:, 1:, None]\n",
    "    isterminal = attn[:, :-1]\n",
    "    print(f\"Tokens shape = {tokens.shape}\")\n",
    "    print(f\"Attn shape = {attn.shape}\")\n",
    "    print(f\"Actions shape = {actions.shape}\")\n",
    "    print(f\"Rewards shape = {rewards.shape}\")\n",
    "    print()\n",
    "    print(f\"Tokens = {tokens}\")\n",
    "    print(f\"Attn = {attn}\")\n",
    "    print(f\"Actions = {actions}\")\n",
    "    print(f\"Rewards = {rewards}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, attn, rewards = batch\n",
    "actions = tokens[:, 1:, None]\n",
    "isterminal = attn[:, :-1]\n",
    "\n",
    "logits, qs, target_qs, vs, _ = self(input_ids=tokens, attention_mask=attn)\n",
    "bsize, ntokens, dsize = logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.044522437723423"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we're picking between 20 tokens uniformly, what do we expect the cross-entropy to be?\n",
    "-1 * np.log(1 / 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honours",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
